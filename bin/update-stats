#!/usr/bin/env python3
import json
import os
import re
import shutil
import socket
import signal
import subprocess
import tempfile
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any
from zoneinfo import ZoneInfo


@dataclass
class CmdResult:
    args: list[str]
    exit_code: int
    stdout: str
    stderr: str
    timeout_s: int


def utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def safe_tail(value: str, max_chars: int) -> str:
    value = value.strip()
    if len(value) <= max_chars:
        return value
    return value[-max_chars:]


def ensure_path_has_dir(env: dict[str, str], directory: str) -> dict[str, str]:
    directory = directory.strip()
    if not directory:
        return env

    current = env.get("PATH", "")
    parts = [p for p in current.split(os.pathsep) if p] if current else []
    if directory in parts:
        return env

    updated = dict(env)
    updated["PATH"] = os.pathsep.join([directory] + parts) if parts else directory
    return updated


def run_cmd(args: list[str], *, timeout_s: int, env: dict[str, str] | None = None) -> CmdResult:
    try:
        proc = subprocess.Popen(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env,
            text=True,
            encoding="utf-8",
            errors="replace",
            start_new_session=True,
        )
    except FileNotFoundError as e:
        return CmdResult(args=args, exit_code=127, stdout="", stderr=str(e), timeout_s=timeout_s)

    try:
        stdout, stderr = proc.communicate(timeout=timeout_s)
        return CmdResult(args=args, exit_code=proc.returncode or 0, stdout=stdout, stderr=stderr, timeout_s=timeout_s)
    except subprocess.TimeoutExpired as e:
        stdout = (e.output or "") if isinstance(e.output, str) else ""
        stderr = (e.stderr or "") if isinstance(e.stderr, str) else ""

        # Kill the whole process group to avoid leaving orphaned children behind.
        try:
            os.killpg(proc.pid, signal.SIGTERM)
        except ProcessLookupError:
            pass

        try:
            more_stdout, more_stderr = proc.communicate(timeout=5)
        except subprocess.TimeoutExpired:
            try:
                os.killpg(proc.pid, signal.SIGKILL)
            except ProcessLookupError:
                pass
            more_stdout, more_stderr = proc.communicate()

        if more_stdout:
            stdout = (stdout or "") + more_stdout
        if more_stderr:
            stderr = (stderr + "\n" if stderr else "") + more_stderr

        stderr = (stderr + "\n" if stderr else "") + f"Timeout after {timeout_s}s"
        return CmdResult(args=args, exit_code=124, stdout=stdout, stderr=stderr, timeout_s=timeout_s)
    except FileNotFoundError as e:
        return CmdResult(args=args, exit_code=127, stdout="", stderr=str(e), timeout_s=timeout_s)


def run_codexbar_claude_guard() -> None:
    guard_path = Path("/usr/local/sbin/codexbar-claude-guard")
    if not guard_path.is_file():
        return
    run_cmd([str(guard_path)], timeout_s=10)


def is_claude_oauth_token_expired(res: CmdResult) -> bool:
    haystack = f"{res.stdout}\n{res.stderr}".lower()
    return "claude oauth token expired" in haystack or "oauth token expired" in haystack


def resolve_claude_bin() -> str:
    """
    systemd timers often run with a minimal PATH that excludes ~/.local/bin.
    Prefer an explicit env override, then PATH lookup, then ~/.local/bin.
    """
    env_bin = os.environ.get("CLAUDE_BIN")
    if env_bin:
        return env_bin

    which_bin = shutil.which("claude")
    if which_bin:
        return which_bin

    local_bin = Path.home() / ".local" / "bin" / "claude"
    if local_bin.is_file():
        return str(local_bin)

    return "claude"


def refresh_claude_oauth_token(*, errors: list[dict[str, Any]], max_stderr_chars: int) -> None:
    """
    Best-effort auth refresh to keep CodexBar's Claude OAuth source working.

    CodexBar suggests: "Run `claude` to refresh." We do this via a tiny
    `claude -p ...` request, which *may spend Claude credits* (model call).
    """
    claude_bin = resolve_claude_bin()
    max_budget_usd = os.environ.get("CLAUDE_REFRESH_MAX_BUDGET_USD", "0.0001")
    timeout_s = int(os.environ.get("CLAUDE_REFRESH_TIMEOUT_S", "60"))

    model = os.environ.get("CLAUDE_REFRESH_MODEL", "sonnet").strip()
    no_session_persistence = os.environ.get("CLAUDE_REFRESH_NO_SESSION_PERSISTENCE", "1").strip().lower() in (
        "1",
        "true",
        "yes",
        "y",
        "on",
    )
    tools_value = os.environ.get("CLAUDE_REFRESH_TOOLS")
    # Default to disabling tools entirely to minimize prompt size/cost.
    if tools_value is None:
        tools_value = ""

    args = [claude_bin, "-p", "ping", "--output-format", "json", "--max-budget-usd", max_budget_usd]
    if model:
        args.extend(["--model", model])
    args.extend(["--tools", tools_value])
    if no_session_persistence:
        args.append("--no-session-persistence")

    res = run_cmd(
        args,
        timeout_s=timeout_s,
    )
    # Claude Code may exit 0 while reporting an auth error in JSON output.
    parsed = None
    last_line = ""
    for line in res.stdout.splitlines()[::-1]:
        if line.strip():
            last_line = line.strip()
            break
    if last_line:
        try:
            parsed = json.loads(last_line)
        except Exception:
            parsed = None

    parsed_is_error = isinstance(parsed, dict) and parsed.get("is_error") is True

    if res.exit_code != 0 or parsed_is_error:
        msg = safe_tail(res.stderr, max_stderr_chars).strip()
        if not msg and isinstance(parsed, dict) and isinstance(parsed.get("result"), str):
            msg = safe_tail(parsed["result"], max_stderr_chars).strip()
        if not msg:
            msg = safe_tail(res.stdout, max_stderr_chars).strip()

        lower = msg.lower()
        if "invalid api key" in lower or "please run /login" in lower:
            msg = f"{msg} (Fix: run `claude setup-token` (preferred) or open `claude` and run `/login` once as the service user.)"
        elif "oauth token" in lower or "authentication_error" in lower:
            msg = f"{msg} (Fix: run `claude setup-token` or open `claude` and re-auth.)"

        errors.append(
            {
                "command": "claude -p ping",
                "exitCode": res.exit_code,
                "message": msg or f"Exit {res.exit_code}",
            }
        )


def annotate_claude_usage_errors(items: list[Any]) -> None:
    """
    CodexBar Claude usage relies on the Claude OAuth token. When it's missing,
    CodexBar returns a generic "No available fetch strategy" error which isn't
    actionable in the dashboard. Add a short fix hint.
    """
    for item in items:
        if not isinstance(item, dict):
            continue
        if item.get("provider") != "claude":
            continue
        err = item.get("error")
        if not isinstance(err, dict):
            continue
        msg = str(err.get("message") or "").strip()
        if not msg:
            continue
        if "no available fetch strategy for claude" in msg.lower():
            err["message"] = (
                "No available fetch strategy for claude. "
                "Fix: run `claude setup-token` once on this machine as the same user that runs the stats updater."
            )


def has_claude_fetch_strategy_error(items: list[Any]) -> bool:
    for item in items:
        if not isinstance(item, dict):
            continue
        if item.get("provider") != "claude":
            continue
        err = item.get("error")
        if not isinstance(err, dict):
            continue
        msg = str(err.get("message") or "").lower()
        if "no available fetch strategy for claude" in msg:
            return True
    return False


def parse_json_arrays(stdout: str, *, errors: list[dict[str, Any]], context: str) -> list[Any]:
    items: list[Any] = []
    for line in stdout.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            parsed = json.loads(line)
        except Exception as e:
            errors.append(
                {
                    "command": "json.parse",
                    "context": context,
                    "exitCode": 0,
                    "message": f"Failed to parse JSON line: {e}",
                }
            )
            continue
        if isinstance(parsed, list):
            items.extend(parsed)
        else:
            items.append(parsed)
    return items


def atomic_write_json(path: Path, payload: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile("w", encoding="utf-8", dir=path.parent, prefix=f".{path.name}.", delete=False) as f:
        tmp_path = Path(f.name)
        json.dump(payload, f, ensure_ascii=False, indent=2)
        f.write("\n")
    os.chmod(tmp_path, 0o644)
    os.replace(tmp_path, path)


def load_history(path: Path) -> list[dict[str, Any]]:
    if not path.is_file():
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, list) else []
    except Exception:
        return []


def load_json(path: Path) -> Any | None:
    if not path.is_file():
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def parse_iso_datetime(value: Any) -> datetime | None:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        if not isinstance(value, bool) and value > 0:
            # Heuristic: treat < 1e12 as seconds, otherwise ms.
            ts = float(value)
            if ts > 1e12:
                ts = ts / 1000.0
            try:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
            except Exception:
                return None
        return None
    if isinstance(value, str):
        s = value.strip()
        if not s:
            return None
        try:
            if s.endswith("Z"):
                s = s[:-1] + "+00:00"
            dt = datetime.fromisoformat(s)
            if dt.tzinfo is None:
                return dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        except Exception:
            return None
    return None


def read_tail_lines(path: Path, *, max_bytes: int) -> list[str]:
    try:
        with path.open("rb") as f:
            f.seek(0, os.SEEK_END)
            end = f.tell()
            start = max(0, end - max_bytes)
            f.seek(start)
            chunk = f.read(end - start)
    except Exception:
        return []

    if start > 0:
        nl = chunk.find(b"\n")
        if nl >= 0:
            chunk = chunk[nl + 1 :]
        else:
            return []

    text = chunk.decode("utf-8", errors="replace")
    return [line for line in text.splitlines() if line.strip()]


def is_claude_model(value: Any) -> bool:
    return isinstance(value, str) and value.startswith("claude-")


def discover_clawdbot_state_dirs() -> list[Path]:
    candidates: list[Path] = []
    for base in (Path("/root"), Path("/home/web"), Path("/root/ClawBridgeGateway/state")):
        if not base.exists():
            continue
        for child in base.glob(".clawdbot*"):
            if child.is_dir():
                candidates.append(child)
        for child in base.glob(".clawdbot-state*"):
            if child.is_dir():
                candidates.append(child)

    out: list[Path] = []
    seen: set[str] = set()
    for d in candidates:
        try:
            key = str(d.resolve())
        except Exception:
            key = str(d)
        if key in seen:
            continue
        seen.add(key)
        out.append(d)
    return out


def find_last_claude_cli_event(*, home: Path, exclude_project_dirs: set[Path], max_files: int, max_bytes: int) -> dict[str, Any] | None:
    projects_dir = home / ".claude" / "projects"
    history_file = home / ".claude" / "history.jsonl"

    candidates: list[Path] = []
    if projects_dir.is_dir():
        candidates.extend(projects_dir.glob("**/*.jsonl"))
    if history_file.is_file():
        candidates.append(history_file)

    candidates = [p for p in candidates if p.is_file() and p.suffix == ".jsonl"]
    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    if max_files > 0:
        candidates = candidates[:max_files]

    best: dict[str, Any] | None = None
    for path in candidates:
        if any(excl in path.parents for excl in exclude_project_dirs):
            continue

        for line in reversed(read_tail_lines(path, max_bytes=max_bytes)):
            try:
                entry = json.loads(line)
            except Exception:
                continue
            if not isinstance(entry, dict):
                continue
            msg = entry.get("message")
            if not isinstance(msg, dict):
                continue
            model = msg.get("model")
            if not is_claude_model(model):
                continue
            ts = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
            if ts is None:
                continue
            item = {
                "tsUtc": ts.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "source": "cli",
                "provider": "claude-cli",
                "model": str(model),
                "path": str(path),
                "cwd": entry.get("cwd") if isinstance(entry.get("cwd"), str) else None,
            }
            if best is None or str(item["tsUtc"]) > str(best.get("tsUtc")):
                best = item
            break

    return best


def find_last_claude_bot_event(*, state_dirs: list[Path], max_files: int, max_bytes: int) -> dict[str, Any] | None:
    candidates: list[Path] = []
    for state_dir in state_dirs:
        agents_dir = state_dir / "agents"
        if not agents_dir.is_dir():
            continue
        candidates.extend(sorted(agents_dir.glob("*/sessions/*.jsonl")))

    candidates.sort(key=lambda p: p.stat().st_mtime if p.exists() else 0, reverse=True)
    if max_files > 0:
        candidates = candidates[:max_files]

    best: dict[str, Any] | None = None
    for path in candidates:
        for line in reversed(read_tail_lines(path, max_bytes=max_bytes)):
            try:
                entry = json.loads(line)
            except Exception:
                continue
            if not isinstance(entry, dict):
                continue
            if entry.get("type") != "message":
                continue
            msg = entry.get("message")
            if not isinstance(msg, dict) or msg.get("role") != "assistant":
                continue

            provider = msg.get("provider") or entry.get("provider")
            model = msg.get("model") or entry.get("model")
            provider_value = str(provider or "").strip().lower()
            if not (is_claude_model(model) or provider_value in ("anthropic", "claude")):
                continue

            ts = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
            if ts is None:
                continue

            item = {
                "tsUtc": ts.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "source": "bots",
                "provider": str(provider) if isinstance(provider, str) else None,
                "model": str(model) if isinstance(model, str) else None,
                "path": str(path),
            }
            if best is None or str(item["tsUtc"]) > str(best.get("tsUtc")):
                best = item
            break

    return best


def compute_last_claude_activity(*, max_files: int, max_bytes: int) -> dict[str, Any]:
    exclude_refresh = {Path("/root/.claude/projects/-home-mlweb-stats-lucheestiy-com")}
    cli_root = find_last_claude_cli_event(
        home=Path("/root"),
        exclude_project_dirs=exclude_refresh,
        max_files=max_files,
        max_bytes=max_bytes,
    )
    cli_web = find_last_claude_cli_event(
        home=Path("/home/web"),
        exclude_project_dirs=set(),
        max_files=max_files,
        max_bytes=max_bytes,
    )
    bots = find_last_claude_bot_event(
        state_dirs=discover_clawdbot_state_dirs(),
        max_files=max_files,
        max_bytes=max_bytes,
    )

    candidates = [c for c in (cli_root, cli_web, bots) if isinstance(c, dict) and isinstance(c.get("tsUtc"), str)]
    overall = None
    if candidates:
        overall = max(candidates, key=lambda d: str(d.get("tsUtc") or ""))

    return {
        "generatedAt": utc_now_iso(),
        "cli": {"root": cli_root, "web": cli_web},
        "bots": bots,
        "overall": overall,
        "excluded": {"refreshProject": "/root/.claude/projects/-home-mlweb-stats-lucheestiy-com"},
    }


def to_ymd_in_tz(dt_utc: datetime, tz_name: str) -> str:
    try:
        tz = ZoneInfo(tz_name)
    except Exception:
        return ""
    try:
        return dt_utc.astimezone(tz).date().isoformat()
    except Exception:
        return ""


def as_finite_number(value: Any) -> float | None:
    if value is None:
        return None
    if isinstance(value, bool):
        return None
    if isinstance(value, (int, float)):
        n = float(value)
        return n if n >= 0 and n != float("inf") and n == n else None
    return None


@lru_cache(maxsize=1)
def minimax_pricing_usd_per_million_tokens() -> dict[str, float]:
    def env_float(name: str, default: float) -> float:
        value = os.environ.get(name)
        if value is None:
            return default
        try:
            parsed = float(value)
        except Exception:
            return default
        return parsed if parsed >= 0 and parsed == parsed and parsed != float("inf") else default

    return {
        "input": env_float("MINIMAX_COST_INPUT_PER_MILLION_USD", 0.3),
        "output": env_float("MINIMAX_COST_OUTPUT_PER_MILLION_USD", 1.2),
        "cacheRead": env_float("MINIMAX_COST_CACHE_READ_PER_MILLION_USD", 0.03),
        "cacheWrite": env_float("MINIMAX_COST_CACHE_WRITE_PER_MILLION_USD", 0.375),
    }


def compute_minimax_cost_usd(
    *,
    input_tokens: float | None,
    output_tokens: float | None,
    cache_read_tokens: float | None,
    cache_write_tokens: float | None,
) -> float:
    pricing = minimax_pricing_usd_per_million_tokens()
    return (
        (input_tokens or 0.0) * pricing["input"]
        + (output_tokens or 0.0) * pricing["output"]
        + (cache_read_tokens or 0.0) * pricing["cacheRead"]
        + (cache_write_tokens or 0.0) * pricing["cacheWrite"]
    ) / 1_000_000.0


def compute_clawdbot_cost_delta(
    *,
    tz_name: str,
    days: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
    provider: str,
    source: str,
    state_dir: Path | None = None,
    match_providers: tuple[str, ...] | None = None,
    match_model_prefixes: tuple[str, ...] | None = None,
) -> dict[str, Any] | None:
    """
    Clawdbot stores per-message usage+cost in ~/.clawdbot*/agents/*/sessions/*.jsonl.
    CodexBar's `cost` scanner doesn't see these, so we merge them into the
    relevant cost provider.
    """
    if state_dir is None:
        state_dir = Path(os.environ.get("CLAWDBOT_STATE_DIR", str(Path.home() / ".clawdbot")))

    provider_out = str(provider or "").strip() or "unknown"
    source_out = str(source or "").strip() or "clawdbot"
    provider_match = tuple(match_providers) if match_providers is not None else (provider_out,)
    provider_match_lower = tuple(str(p).strip().lower() for p in provider_match if str(p).strip())
    model_prefixes = tuple(match_model_prefixes) if match_model_prefixes is not None else tuple()

    agents_dir = state_dir / "agents"
    if not agents_dir.is_dir():
        return None

    now_utc = datetime.now(timezone.utc)
    # Include "today" as one of the days (align with dashboard 30d windows).
    since_utc = now_utc - timedelta(days=max(1, days) - 1)
    since_ts_s = since_utc.timestamp()

    daily: dict[str, dict[str, Any]] = {}
    totals: dict[str, float] = {
        "inputTokens": 0.0,
        "outputTokens": 0.0,
        "cacheReadTokens": 0.0,
        "cacheCreationTokens": 0.0,
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    session_files = sorted(agents_dir.glob("*/sessions/*.jsonl"))
    for path in session_files:
        if path.name.endswith(".jsonl.lock"):
            continue
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < since_ts_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue
                    if not isinstance(entry, dict):
                        continue
                    if entry.get("type") != "message":
                        continue
                    msg = entry.get("message")
                    if not isinstance(msg, dict):
                        continue
                    if msg.get("role") != "assistant":
                        continue

                    provider = msg.get("provider") or entry.get("provider")
                    model = msg.get("model") or entry.get("model")
                    provider_value = str(provider or "").strip().lower()
                    model_value = str(model or "")
                    if provider_match_lower and provider_value not in provider_match_lower and not any(
                        model_value.startswith(prefix) for prefix in model_prefixes
                    ):
                        continue

                    usage = msg.get("usage") or entry.get("usage")
                    if not isinstance(usage, dict):
                        continue

                    dt_utc = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
                    if dt_utc is None or dt_utc.timestamp() < since_ts_s:
                        continue
                    day = to_ymd_in_tz(dt_utc, tz_name)
                    if not day:
                        continue

                    bucket = daily.get(day)
                    if bucket is None:
                        bucket = {
                            "date": day,
                            "inputTokens": 0.0,
                            "outputTokens": 0.0,
                            "cacheReadTokens": 0.0,
                            "cacheCreationTokens": 0.0,
                            "totalTokens": 0.0,
                            "totalCost": 0.0,
                            "missingCostEntries": 0.0,
                            "modelsUsed": set(),
                            "modelCosts": {},
                        }
                        daily[day] = bucket

                    input_tokens = as_finite_number(usage.get("input"))
                    output_tokens = as_finite_number(usage.get("output"))
                    cache_read = as_finite_number(usage.get("cacheRead"))
                    cache_write = as_finite_number(usage.get("cacheWrite"))
                    total_tokens = as_finite_number(usage.get("totalTokens")) or as_finite_number(usage.get("total"))
                    if total_tokens is None:
                        total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                    cost_total = None
                    if provider_value == "minimax":
                        if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                            bucket["missingCostEntries"] += 1.0
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0
                        else:
                            cost_total = compute_minimax_cost_usd(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                cache_read_tokens=cache_read,
                                cache_write_tokens=cache_write,
                            )
                    else:
                        cost = usage.get("cost")
                        if isinstance(cost, dict):
                            cost_total = as_finite_number(cost.get("total"))
                        if cost_total is None:
                            bucket["missingCostEntries"] += 1.0
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0

                    bucket["inputTokens"] += input_tokens or 0.0
                    bucket["outputTokens"] += output_tokens or 0.0
                    bucket["cacheReadTokens"] += cache_read or 0.0
                    bucket["cacheCreationTokens"] += cache_write or 0.0
                    bucket["totalTokens"] += total_tokens
                    bucket["totalCost"] += cost_total

                    totals["inputTokens"] += input_tokens or 0.0
                    totals["outputTokens"] += output_tokens or 0.0
                    totals["cacheReadTokens"] += cache_read or 0.0
                    totals["cacheCreationTokens"] += cache_write or 0.0
                    totals["totalTokens"] += total_tokens
                    totals["totalCost"] += cost_total

                    if isinstance(model, str) and model:
                        bucket["modelsUsed"].add(model)
                        model_costs = bucket["modelCosts"]
                        prev_cost = model_costs.get(model, 0.0)
                        model_costs[model] = prev_cost + cost_total
        except Exception as e:
            errors.append(
                {
                    "command": "clawdbot transcripts scan",
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Clawdbot sessions",
                }
            )

    if not daily:
        return None

    daily_items: list[dict[str, Any]] = []
    for day in sorted(daily.keys()):
        b = daily[day]
        model_breakdowns = [
            {"modelName": name, "cost": cost} for name, cost in sorted(b["modelCosts"].items(), key=lambda kv: kv[1], reverse=True)
        ]
        models_used = sorted(b["modelsUsed"])
        daily_items.append(
            {
                "date": b["date"],
                "inputTokens": b["inputTokens"],
                "outputTokens": b["outputTokens"],
                "cacheReadTokens": b["cacheReadTokens"],
                "cacheCreationTokens": b["cacheCreationTokens"],
                "totalTokens": b["totalTokens"],
                "totalCost": b["totalCost"],
                "modelBreakdowns": model_breakdowns,
                "modelsUsed": models_used,
            }
            )

    return {
        "updatedAt": utc_now_iso(),
        "provider": provider_out,
        "source": source_out,
        "totals": {
            "inputTokens": totals["inputTokens"],
            "outputTokens": totals["outputTokens"],
            "cacheReadTokens": totals["cacheReadTokens"],
            "cacheCreationTokens": totals["cacheCreationTokens"],
            "totalTokens": totals["totalTokens"],
            "totalCost": totals["totalCost"],
        },
        "daily": daily_items,
        "last30DaysTokens": totals["totalTokens"],
        "last30DaysCostUSD": totals["totalCost"],
    }


def compute_clawdbot_window_usage(
    *,
    provider: str,
    state_dir: Path,
    window_minutes: int,
    budget_usd: float,
    reset_offset_minutes: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
) -> dict[str, Any] | None:
    """
    Best-effort "usage" for providers that don't have a CodexBar usage fetch
    strategy on this host. Derives a 5h window from Clawdbot transcripts and
    treats the spend within that window as % of a configured budget.
    """
    agents_dir = state_dir / "agents"
    if not agents_dir.is_dir():
        return None

    provider_value = str(provider or "").strip() or "unknown"
    window_minutes = int(window_minutes or 0)
    if window_minutes <= 0:
        window_minutes = 300

    budget = float(budget_usd or 0.0)
    if budget < 0 or budget != budget:
        budget = 0.0

    now_utc = datetime.now(timezone.utc)
    now_ts = now_utc.timestamp()
    window_s = float(window_minutes) * 60.0
    offset_s = float(int(reset_offset_minutes or 0) * 60)
    window_index = int((now_ts - offset_s) // window_s)
    window_start_s = window_index * window_s + offset_s
    window_end_s = window_start_s + window_s

    window_end_dt = datetime.fromtimestamp(window_end_s, tz=timezone.utc)
    resets_at = window_end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    totals: dict[str, float] = {
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    session_files = sorted(agents_dir.glob("*/sessions/*.jsonl"))
    for path in session_files:
        if path.name.endswith(".jsonl.lock"):
            continue
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < window_start_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue
                    if not isinstance(entry, dict):
                        continue
                    if entry.get("type") != "message":
                        continue
                    msg = entry.get("message")
                    if not isinstance(msg, dict):
                        continue
                    if msg.get("role") != "assistant":
                        continue

                    p = msg.get("provider") or entry.get("provider")
                    if str(p or "").strip().lower() != provider_value.lower():
                        continue

                    usage = msg.get("usage") or entry.get("usage")
                    if not isinstance(usage, dict):
                        continue

                    dt_utc = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
                    if dt_utc is None:
                        continue
                    ts = dt_utc.timestamp()
                    if ts < window_start_s or ts >= window_end_s:
                        continue

                    input_tokens = as_finite_number(usage.get("input"))
                    output_tokens = as_finite_number(usage.get("output"))
                    cache_read = as_finite_number(usage.get("cacheRead"))
                    cache_write = as_finite_number(usage.get("cacheWrite"))
                    total_tokens = as_finite_number(usage.get("totalTokens")) or as_finite_number(usage.get("total"))
                    if total_tokens is None:
                        total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                    cost_total = None
                    if provider_value.lower() == "minimax":
                        if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0
                        else:
                            cost_total = compute_minimax_cost_usd(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                cache_read_tokens=cache_read,
                                cache_write_tokens=cache_write,
                            )
                    else:
                        cost = usage.get("cost")
                        if isinstance(cost, dict):
                            cost_total = as_finite_number(cost.get("total"))
                        if cost_total is None:
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0

                    totals["totalTokens"] += total_tokens
                    totals["totalCost"] += cost_total
        except Exception as e:
            errors.append(
                {
                    "command": "clawdbot transcripts scan",
                    "provider": provider_value,
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Clawdbot sessions",
                }
            )

    used_percent = 0.0
    if budget > 0:
        used_percent = (totals["totalCost"] / budget) * 100.0

    login_method = f"Coding subscription (${budget:g})" if budget > 0 else "Coding subscription"

    return {
        "updatedAt": utc_now_iso(),
        "identity": {
            "loginMethod": login_method,
            "providerID": provider_value,
        },
        "primary": {
            "usedPercent": used_percent,
            "resetsAt": resets_at,
            "windowMinutes": window_minutes,
        },
        "secondary": None,
        "tertiary": None,
        "loginMethod": login_method,
        "window": {
            "budgetUSD": budget,
            "totalCostUSD": totals["totalCost"],
            "totalTokens": totals["totalTokens"],
            "missingCostEntries": totals["missingCostEntries"],
            "resetOffsetMinutes": int(reset_offset_minutes or 0),
        },
    }


def compute_direct_claude_minimax_usage(
    *,
    window_minutes: int,
    budget_usd: float,
    reset_offset_minutes: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
) -> dict[str, Any] | None:
    """
    Track MiniMax usage from direct Claude CLI sessions (not through Clawdbot).
    Scans ~/.claude/ directory for JSONL files with MiniMax model usage.
    """
    window_minutes = int(window_minutes or 0)
    if window_minutes <= 0:
        window_minutes = 300

    budget = float(budget_usd or 0.0)
    if budget < 0 or budget != budget:
        budget = 0.0

    now_utc = datetime.now(timezone.utc)
    now_ts = now_utc.timestamp()
    window_s = float(window_minutes) * 60.0
    offset_s = float(int(reset_offset_minutes or 0) * 60)
    window_index = int((now_ts - offset_s) // window_s)
    window_start_s = window_index * window_s + offset_s
    window_end_s = window_start_s + window_s

    window_end_dt = datetime.fromtimestamp(window_end_s, tz=timezone.utc)
    resets_at = window_end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    totals: dict[str, float] = {
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    # Scan ~/.claude/ directory and any custom homeDirs used by Clawdbot
    claude_dirs = [Path.home() / ".claude"]

    # Add custom homeDir used by ClaudeMinimaxBot
    minimax_custom_home = Path("/home/web/.clawdbot-claude-bypass-home-minimax/.claude")
    if minimax_custom_home.is_dir():
        claude_dirs.append(minimax_custom_home)

    # Add web user's claude directory
    web_claude = Path("/home/web/.claude")
    if web_claude.is_dir():
        claude_dirs.append(web_claude)

    # Add droid (factory) sessions directory
    factory_sessions = Path.home() / ".factory"

    # Find all JSONL files in all claude directories
    jsonl_files = []
    for claude_dir in claude_dirs:
        if not claude_dir.is_dir():
            continue
        jsonl_files.extend(claude_dir.glob("*.jsonl"))
        jsonl_files.extend(claude_dir.glob("projects/*/*.jsonl"))
        # Also scan subagent directories
        jsonl_files.extend(claude_dir.glob("projects/*/*/subagents/*.jsonl"))

    # Also scan droid/factory sessions (different structure: sessions/*/*.jsonl)
    if factory_sessions.is_dir():
        jsonl_files.extend(factory_sessions.glob("sessions/*/*.jsonl"))

    if not jsonl_files:
        return None

    for path in sorted(jsonl_files):
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < window_start_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue

                    # Check for MiniMax model in different entry formats
                    model = None
                    usage = None
                    timestamp = None

                    # Format 1: Direct message entry
                    if entry.get("type") == "assistant" and isinstance(entry.get("message"), dict):
                        message = entry.get("message", {})
                        model = message.get("model")
                        usage = message.get("usage")
                        timestamp = entry.get("timestamp")
                    # Format 2: Project session entry
                    elif isinstance(entry.get("message"), dict):
                        message = entry.get("message", {})
                        model = message.get("model")
                        usage = message.get("usage")
                        timestamp = entry.get("timestamp")

                    # Check if this is MiniMax
                    if model and "minimax" in str(model).lower():
                        if not isinstance(usage, dict):
                            continue

                        dt_utc = parse_iso_datetime(timestamp)
                        if dt_utc is None:
                            continue
                        ts = dt_utc.timestamp()
                        if ts < window_start_s or ts >= window_end_s:
                            continue

                        # Extract tokens (different format from Clawdbot)
                        input_tokens = as_finite_number(usage.get("input_tokens"))
                        output_tokens = as_finite_number(usage.get("output_tokens"))
                        cache_read = as_finite_number(usage.get("cache_read_input_tokens"))
                        cache_write = as_finite_number(usage.get("cache_creation_input_tokens"))
                        total_tokens = as_finite_number(usage.get("total_tokens"))

                        if total_tokens is None:
                            total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                        # Calculate cost using MiniMax pricing
                        if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0
                        else:
                            cost_total = compute_minimax_cost_usd(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                cache_read_tokens=cache_read,
                                cache_write_tokens=cache_write,
                            )

                        totals["totalTokens"] += total_tokens
                        totals["totalCost"] += cost_total

        except Exception as e:
            errors.append(
                {
                    "command": "direct claude cli scan",
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Claude CLI logs",
                }
            )

    # Also scan droid/factory .settings.json files (usage stored separately from sessions)
    if factory_sessions.is_dir():
        try:
            for settings_path in factory_sessions.glob("sessions/*/*.settings.json"):
                try:
                    st = settings_path.stat()
                except Exception:
                    continue
                if st.st_mtime < window_start_s:
                    continue

                try:
                    with open(settings_path, "r", encoding="utf-8", errors="replace") as f:
                        data = json.load(f)
                except Exception:
                    continue

                if not isinstance(data, dict):
                    continue

                model = data.get("model", "")
                if not model or "minimax" not in str(model).lower():
                    continue

                token_usage = data.get("tokenUsage")
                if not isinstance(token_usage, dict):
                    continue

                # Get timestamp from providerLockTimestamp or file mtime
                timestamp = data.get("providerLockTimestamp")
                if timestamp:
                    dt_utc = parse_iso_datetime(timestamp)
                    if dt_utc:
                        ts = dt_utc.timestamp()
                        if ts < window_start_s or ts >= window_end_s:
                            continue

                # Extract tokens (droid format)
                input_tokens = as_finite_number(token_usage.get("inputTokens"))
                output_tokens = as_finite_number(token_usage.get("outputTokens"))
                cache_read = as_finite_number(token_usage.get("cacheReadTokens"))
                cache_write = as_finite_number(token_usage.get("cacheCreationTokens"))

                total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                    totals["missingCostEntries"] += 1.0
                    cost_total = 0.0
                else:
                    cost_total = compute_minimax_cost_usd(
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        cache_read_tokens=cache_read,
                        cache_write_tokens=cache_write,
                    )

                totals["totalTokens"] += total_tokens
                totals["totalCost"] += cost_total

        except Exception as e:
            errors.append(
                {
                    "command": "droid factory scan",
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan droid sessions",
                }
            )

    used_percent = 0.0
    if budget > 0:
        used_percent = (totals["totalCost"] / budget) * 100.0

    login_method = f"Coding subscription (${budget:g})" if budget > 0 else "Coding subscription"

    return {
        "updatedAt": utc_now_iso(),
        "identity": {
            "loginMethod": login_method,
            "providerID": "minimax",
        },
        "primary": {
            "usedPercent": used_percent,
            "resetsAt": resets_at,
            "windowMinutes": window_minutes,
        },
        "secondary": None,
        "tertiary": None,
        "loginMethod": login_method,
        "window": {
            "budgetUSD": budget,
            "totalCostUSD": totals["totalCost"],
            "totalTokens": totals["totalTokens"],
            "missingCostEntries": totals["missingCostEntries"],
            "resetOffsetMinutes": int(reset_offset_minutes or 0),
        },
    }


def merge_cost_items_by_provider(base: list[Any], delta: dict[str, Any], provider: str) -> None:
    if not delta:
        return

    target = None
    for item in base:
        if isinstance(item, dict) and item.get("provider") == provider:
            target = item
            break

    if target is None:
        base.append(delta)
        return

    # Merge totals + 30d totals
    target_totals = target.get("totals")
    if not isinstance(target_totals, dict):
        target_totals = {}
        target["totals"] = target_totals
    delta_totals = delta.get("totals")
    if isinstance(delta_totals, dict):
        for key in ["inputTokens", "outputTokens", "cacheReadTokens", "cacheCreationTokens", "totalTokens", "totalCost"]:
            a = as_finite_number(target_totals.get(key)) or 0.0
            b = as_finite_number(delta_totals.get(key)) or 0.0
            target_totals[key] = a + b

    for key in ["last30DaysTokens", "last30DaysCostUSD"]:
        a = as_finite_number(target.get(key)) or 0.0
        b = as_finite_number(delta.get(key)) or 0.0
        target[key] = a + b

    # Mark combined source to avoid confusion.
    base_source = str(target.get("source") or "").strip() or "local"
    delta_source = str(delta.get("source") or "").strip()
    if delta_source and delta_source not in base_source:
        target["source"] = f"{base_source}+{delta_source}"

    # Merge daily entries by date
    target_daily = target.get("daily")
    if not isinstance(target_daily, list):
        target_daily = []
        target["daily"] = target_daily
    by_date: dict[str, dict[str, Any]] = {}
    for d in target_daily:
        if isinstance(d, dict) and isinstance(d.get("date"), str):
            by_date[d["date"]] = d

    delta_daily = delta.get("daily")
    if isinstance(delta_daily, list):
        for d in delta_daily:
            if not isinstance(d, dict) or not isinstance(d.get("date"), str):
                continue
            date = d["date"]
            existing = by_date.get(date)
            if existing is None:
                target_daily.append(d)
                by_date[date] = d
                continue

            for key in ["inputTokens", "outputTokens", "cacheReadTokens", "cacheCreationTokens", "totalTokens", "totalCost"]:
                a = as_finite_number(existing.get(key)) or 0.0
                b = as_finite_number(d.get(key)) or 0.0
                existing[key] = a + b

            # Merge modelsUsed
            models_used = set(existing.get("modelsUsed") or []) if isinstance(existing.get("modelsUsed"), list) else set()
            models_used.update(d.get("modelsUsed") or [])
            existing["modelsUsed"] = sorted([m for m in models_used if isinstance(m, str) and m])

            # Merge modelBreakdowns
            def to_cost_map(items: Any) -> dict[str, float]:
                out: dict[str, float] = {}
                if not isinstance(items, list):
                    return out
                for mb in items:
                    if not isinstance(mb, dict):
                        continue
                    name = mb.get("modelName")
                    cost = as_finite_number(mb.get("cost")) or 0.0
                    if isinstance(name, str) and name:
                        out[name] = out.get(name, 0.0) + cost
                return out

            merged = to_cost_map(existing.get("modelBreakdowns"))
            for name, cost in to_cost_map(d.get("modelBreakdowns")).items():
                merged[name] = merged.get(name, 0.0) + cost
            existing["modelBreakdowns"] = [
                {"modelName": name, "cost": cost} for name, cost in sorted(merged.items(), key=lambda kv: kv[1], reverse=True)
            ]

    target_daily.sort(key=lambda d: d.get("date") or "")


def fill_missing_cost_days(cost_items: list[Any], *, tz_name: str, days: int) -> None:
    """
    CodexBar cost output omits days with 0 usage, which makes charts/ranges
    look like data is missing (especially near "today"). Fill explicit zero
    entries for the last N days in the target timezone.
    """
    if not isinstance(days, int) or days <= 0:
        return

    try:
        tz = ZoneInfo(tz_name)
    except Exception:
        return

    now_utc = datetime.now(timezone.utc)
    end_date = now_utc.astimezone(tz).date()
    start_date = end_date - timedelta(days=days - 1)
    required_dates = [(start_date + timedelta(days=i)).isoformat() for i in range(days)]
    required_set = set(required_dates)

    for item in cost_items:
        if not isinstance(item, dict):
            continue
        daily = item.get("daily")
        if not isinstance(daily, list):
            continue

        by_date: dict[str, dict[str, Any]] = {}
        extras: list[dict[str, Any]] = []
        for d in daily:
            if not isinstance(d, dict):
                continue
            date = d.get("date")
            if not isinstance(date, str) or not date:
                continue
            if date in required_set:
                # Keep the first entry for each day (should be unique after merges).
                by_date.setdefault(date, d)
            else:
                extras.append(d)

        filled: list[dict[str, Any]] = []
        for date in required_dates:
            existing = by_date.get(date)
            if existing is not None:
                filled.append(existing)
                continue

            filled.append(
                {
                    "date": date,
                    "inputTokens": 0.0,
                    "outputTokens": 0.0,
                    "cacheReadTokens": 0.0,
                    "cacheCreationTokens": 0.0,
                    "totalTokens": 0.0,
                    "totalCost": 0.0,
                    "modelBreakdowns": [],
                    "modelsUsed": [],
                }
            )

        merged_daily = extras + filled
        merged_daily.sort(key=lambda d: str(d.get("date") or ""))
        item["daily"] = merged_daily


def update_hourly_history(data_dir: Path, usage_items: list[dict[str, Any]], generated_at: str, current_codex_account: str | None = None) -> None:
    """
    Track hourly usage history for heatmap visualization.
    Calculates delta (activity) by comparing current values to previous run.
    Each entry: {ts, hour, day, provider, account, activity}
    Keep last 30 days of data.
    """
    history_path = data_dir / "history.json"
    state_path = data_dir / ".history_state.json"

    history = load_history(history_path)
    prev_state = load_history(state_path) if state_path.is_file() else {}
    if isinstance(prev_state, list):
        prev_state = {}

    # Use local time for hour/day display
    now_local = datetime.now()
    now = datetime.now(timezone.utc)
    hour = now_local.hour
    day = now_local.strftime("%Y-%m-%d")

    current_state: dict[str, Any] = {}

    # Extract usage percentages and calculate deltas
    for item in usage_items:
        if not isinstance(item, dict):
            continue

        provider = item.get("provider", "unknown")
        account = item.get("codexAuthAccount", "")

        usage = item.get("usage", {})
        if not isinstance(usage, dict):
            continue

        primary = usage.get("primary", {})
        session_pct = primary.get("usedPercent") if isinstance(primary, dict) else None

        if session_pct is None:
            continue

        key = f"{provider}|{account}"
        # Always update state for all accounts (needed for accurate delta calculation on switch)
        current_state[key] = {"sessionPct": session_pct, "ts": generated_at}

        # Calculate activity delta
        prev = prev_state.get(key, {})
        prev_pct = prev.get("sessionPct")

        activity = 0.0
        if prev_pct is not None and session_pct is not None:
            # If session reset (current < previous), count current as new activity
            if session_pct < prev_pct - 0.5:  # Allow small tolerance for reset detection
                activity = session_pct
            else:
                activity = session_pct - prev_pct
        elif session_pct is not None and session_pct > 0:
            # First run or no previous data - if there's usage, record it
            activity = session_pct

        # For codex provider, only record history for the current active account
        if provider == "codex" and current_codex_account and account != current_codex_account:
            continue

        # Only record if there was meaningful activity (> 0.1% to avoid noise)
        if activity > 0.1:
            entry = {
                "ts": generated_at,
                "hour": hour,
                "day": day,
                "provider": provider,
                "account": account,
                "activity": round(activity, 1),
            }
            history.append(entry)

    # Save current state for next comparison
    atomic_write_json(state_path, current_state)

    # Prune entries older than 30 days
    cutoff = (now - timedelta(days=30)).strftime("%Y-%m-%d")
    history = [e for e in history if e.get("day", "") >= cutoff]

    atomic_write_json(history_path, history)


def parse_accounts_env(value: str) -> list[str]:
    value = value.strip()
    if not value:
        return []
    return [part for part in re.split(r"[,\s]+", value) if part]


def remove_key_recursive(obj: Any, key: str) -> None:
    if isinstance(obj, dict):
        if key in obj:
            obj.pop(key, None)
        for v in list(obj.values()):
            remove_key_recursive(v, key)
    elif isinstance(obj, list):
        for v in obj:
            remove_key_recursive(v, key)


def main() -> int:
    os.umask(0o022)

    base_dir = Path(__file__).resolve().parent.parent
    data_dir = base_dir / "public" / "data"

    base_env = ensure_path_has_dir(dict(os.environ), str(Path.home() / ".local" / "bin"))

    codexbar_bin = os.environ.get("CODEXBAR_BIN", "/usr/local/bin/codexbar")
    usage_timeout_s = int(os.environ.get("USAGE_TIMEOUT_S", "60"))
    cost_timeout_s = int(os.environ.get("COST_TIMEOUT_S", "180"))
    max_stderr_chars = int(os.environ.get("MAX_STDERR_CHARS", "4000"))

    # Local Claude activity (read-only) is used to decide whether a costly
    # `claude -p ...` refresh is actually worth doing.
    try:
        claude_activity_max_files = int(os.environ.get("CLAUDE_ACTIVITY_MAX_FILES", "250"))
    except Exception:
        claude_activity_max_files = 250
    try:
        claude_activity_max_bytes = int(os.environ.get("CLAUDE_ACTIVITY_MAX_BYTES", "256000"))
    except Exception:
        claude_activity_max_bytes = 256000
    claude_activity = compute_last_claude_activity(max_files=claude_activity_max_files, max_bytes=claude_activity_max_bytes)

    # Claude refresh gating state (stored locally, no secrets).
    claude_refresh_state_path = data_dir / ".claude-refresh-state.json"
    claude_refresh_state = load_json(claude_refresh_state_path)
    if not isinstance(claude_refresh_state, dict):
        claude_refresh_state = {}
    claude_refresh_policy = str(os.environ.get("CLAUDE_REFRESH_POLICY", "if-new-activity")).strip().lower() or "if-new-activity"
    try:
        claude_refresh_min_interval_s = int(os.environ.get("CLAUDE_REFRESH_MIN_INTERVAL_S", "1800"))
    except Exception:
        claude_refresh_min_interval_s = 1800
    if claude_refresh_min_interval_s < 0:
        claude_refresh_min_interval_s = 0
    claude_refresh_meta: dict[str, Any] = {
        "policy": claude_refresh_policy,
        "minIntervalSec": claude_refresh_min_interval_s,
        "attempted": False,
        "skipped": False,
        "reason": "",
        "lastActivityAt": (claude_activity.get("overall") or {}).get("tsUtc") if isinstance(claude_activity, dict) else None,
        "lastUsageOkAt": claude_refresh_state.get("lastClaudeUsageOkAt"),
        "lastRefreshAttemptAt": claude_refresh_state.get("lastClaudeRefreshAttemptAt"),
    }

    codex_accounts_dir = Path(os.environ.get("CODEX_ACCOUNTS_DIR", str(Path.home() / ".codex" / "accounts")))
    codex_config_toml = Path(os.environ.get("CODEX_CONFIG_TOML", str(Path.home() / ".codex" / "config.toml")))
    selected_accounts = parse_accounts_env(os.environ.get("CODEX_AUTH_ACCOUNTS", ""))

    generated_at = utc_now_iso()
    hostname = socket.gethostname()

    # Get current active codex account
    current_codex_account = None
    current_file = Path.home() / ".codex" / "current"
    if current_file.is_file():
        try:
            current_codex_account = current_file.read_text().strip()
        except Exception:
            pass

    errors: list[dict[str, Any]] = []
    usage_items: list[dict[str, Any]] = []
    cost_items: list[Any] = []

    exit_codes: dict[str, Any] = {"usage": {"codex": {}, "claude": None, "gemini": None, "minimax": None}, "cost": None}

    # Support multiple clawdbot state directories for minimax
    minimax_state_dirs_env = os.environ.get("CLAWDBOT_MINIMAX_STATE_DIRS", "")
    if minimax_state_dirs_env.strip():
        minimax_state_dirs = [Path(p.strip()) for p in minimax_state_dirs_env.split(":") if p.strip()]
    else:
        # Default: scan common minimax clawdbot profiles
        minimax_state_dirs = [
            Path(os.environ.get("CLAWDBOT_MINIMAX_STATE_DIR", str(Path.home() / ".clawdbot-clawdminimaxbot"))),
            Path.home() / ".clawdbot-minimax",
        ]
    try:
        minimax_session_budget_usd = float(os.environ.get("MINIMAX_SESSION_BUDGET_USD", "50"))
    except Exception:
        minimax_session_budget_usd = 50.0
    try:
        minimax_session_window_minutes = int(os.environ.get("MINIMAX_SESSION_WINDOW_MINUTES", "300"))
    except Exception:
        minimax_session_window_minutes = 300
    try:
        minimax_session_reset_offset_minutes = int(os.environ.get("MINIMAX_SESSION_RESET_OFFSET_MINUTES", "0"))
    except Exception:
        minimax_session_reset_offset_minutes = 0

    # --- Codex: multi-account via codex-auth profiles (isolated HOME per profile; no global switching).
    all_account_files = sorted(codex_accounts_dir.glob("*.json")) if codex_accounts_dir.is_dir() else []
    if selected_accounts:
        by_name = {p.stem: p for p in all_account_files}
        account_files: list[Path] = []
        for name in selected_accounts:
            p = by_name.get(name)
            if p is None:
                errors.append(
                    {
                        "command": "codexbar usage --provider codex",
                        "profile": name,
                        "exitCode": 1,
                        "message": f"codex-auth profile not found: {codex_accounts_dir}/{name}.json",
                    }
                )
                continue
            account_files.append(p)
    else:
        account_files = all_account_files

    if not account_files:
        errors.append(
            {
                "command": "codexbar usage --provider codex",
                "exitCode": 1,
                "message": f"No codex-auth accounts found in {codex_accounts_dir}",
            }
        )

    for account_file in account_files:
        profile = account_file.stem
        with tempfile.TemporaryDirectory(prefix="codex-home-") as tmp_home:
            tmp_home_path = Path(tmp_home)
            tmp_codex_dir = tmp_home_path / ".codex"
            tmp_codex_dir.mkdir(parents=True, exist_ok=True)
            (tmp_codex_dir / "auth.json").symlink_to(account_file)
            if codex_config_toml.is_file():
                shutil.copy2(codex_config_toml, tmp_codex_dir / "config.toml")

            env = dict(base_env)
            env["HOME"] = tmp_home

            res = run_cmd(
                [codexbar_bin, "usage", "--provider", "codex", "--format", "json", "--json-only"],
                timeout_s=usage_timeout_s,
                env=env,
            )

        exit_codes["usage"]["codex"][profile] = res.exit_code

        if res.stderr.strip() or res.exit_code != 0:
            errors.append(
                {
                    "command": "codexbar usage --provider codex",
                    "profile": profile,
                    "exitCode": res.exit_code,
                    "message": safe_tail(res.stderr, max_stderr_chars) or f"Exit {res.exit_code}",
                }
            )

        parsed = parse_json_arrays(res.stdout, errors=errors, context=f"usage.codex.{profile}")
        for item in parsed:
            if isinstance(item, dict) and item.get("provider") != "cli":
                item["codexAuthAccount"] = profile
                usage_items.append(item)

    # --- Claude usage (OAuth, configured in ~/.codexbar/config.json)
    claude_res = run_cmd(
        [codexbar_bin, "usage", "--provider", "claude", "--format", "json", "--json-only"],
        timeout_s=usage_timeout_s,
        env=base_env,
    )
    claude_parsed = parse_json_arrays(claude_res.stdout, errors=errors, context="usage.claude")
    claude_needs_refresh = is_claude_oauth_token_expired(claude_res) or has_claude_fetch_strategy_error(claude_parsed)
    if claude_needs_refresh:
        now_utc = datetime.now(timezone.utc)
        last_activity_ts = None
        if isinstance(claude_activity, dict):
            overall = claude_activity.get("overall")
            if isinstance(overall, dict):
                last_activity_ts = parse_iso_datetime(overall.get("tsUtc"))
        last_ok_ts = parse_iso_datetime(claude_refresh_state.get("lastClaudeUsageOkAt"))
        last_attempt_ts = parse_iso_datetime(claude_refresh_state.get("lastClaudeRefreshAttemptAt"))

        has_new_activity = last_activity_ts is not None and (last_ok_ts is None or last_activity_ts > last_ok_ts)
        recent_attempt = (
            last_attempt_ts is not None and (now_utc - last_attempt_ts).total_seconds() < float(claude_refresh_min_interval_s)
        )

        allow_refresh = False
        reason = ""

        if claude_refresh_policy in ("never", "off", "disabled", "0", "false"):
            allow_refresh = False
            reason = "policy=never"
        elif recent_attempt:
            allow_refresh = False
            reason = f"recent refresh attempt ({claude_refresh_min_interval_s}s cooldown)"
        elif claude_refresh_policy in ("always", "force"):
            allow_refresh = True
            reason = "policy=always"
        elif claude_refresh_policy in ("on-activity", "activity-window"):
            try:
                window_min = int(os.environ.get("CLAUDE_REFRESH_ACTIVITY_WINDOW_MINUTES", "360"))
            except Exception:
                window_min = 360
            if window_min < 0:
                window_min = 0
            if last_activity_ts is None:
                allow_refresh = False
                reason = "no local Claude activity found"
            else:
                age_s = (now_utc - last_activity_ts).total_seconds()
                allow_refresh = age_s <= float(window_min) * 60.0
                reason = f"activity age {int(age_s)}s (window {window_min}m)"
        else:
            # Default: refresh only if there was local Claude activity since last successful Claude usage fetch.
            allow_refresh = bool(has_new_activity)
            if last_activity_ts is None:
                reason = "no local Claude activity found"
            elif has_new_activity:
                reason = "new local Claude activity since last ok usage fetch"
            else:
                reason = "no new local Claude activity since last ok usage fetch"

        if allow_refresh:
            claude_refresh_meta["attempted"] = True
            claude_refresh_meta["reason"] = reason
            claude_refresh_state["lastClaudeRefreshAttemptAt"] = utc_now_iso()
            claude_refresh_state["lastClaudeRefreshAttemptReason"] = reason
            refresh_claude_oauth_token(errors=errors, max_stderr_chars=max_stderr_chars)
            claude_res = run_cmd(
                [codexbar_bin, "usage", "--provider", "claude", "--format", "json", "--json-only"],
                timeout_s=usage_timeout_s,
                env=base_env,
            )
        else:
            claude_refresh_meta["skipped"] = True
            claude_refresh_meta["reason"] = reason
    exit_codes["usage"]["claude"] = claude_res.exit_code
    claude_parsed = parse_json_arrays(claude_res.stdout, errors=errors, context="usage.claude")
    annotate_claude_usage_errors(claude_parsed)
    claude_has_provider_error = any(isinstance(item, dict) and isinstance(item.get("error"), dict) for item in claude_parsed)
    if claude_res.stderr.strip() or (claude_res.exit_code != 0 and not claude_has_provider_error):
        errors.append(
            {
                "command": "codexbar usage --provider claude",
                "exitCode": claude_res.exit_code,
                "message": safe_tail(claude_res.stderr, max_stderr_chars) or f"Exit {claude_res.exit_code}",
            }
        )
    for item in claude_parsed:
        if isinstance(item, dict) and item.get("provider") != "cli":
            usage_items.append(item)

    # Record last known-good Claude usage fetch time (for refresh gating).
    if not claude_has_provider_error and claude_res.exit_code == 0:
        claude_refresh_state["lastClaudeUsageOkAt"] = generated_at
        if claude_refresh_meta.get("attempted") is True:
            claude_refresh_state["lastClaudeRefreshOkAt"] = generated_at

    # --- Gemini usage (API via Gemini CLI credentials)
    gemini_res = run_cmd(
        [codexbar_bin, "usage", "--provider", "gemini", "--source", "api", "--format", "json", "--json-only"],
        timeout_s=usage_timeout_s,
        env=base_env,
    )
    exit_codes["usage"]["gemini"] = gemini_res.exit_code
    gemini_parsed = parse_json_arrays(gemini_res.stdout, errors=errors, context="usage.gemini")
    gemini_has_provider_error = any(isinstance(item, dict) and isinstance(item.get("error"), dict) for item in gemini_parsed)
    if gemini_res.stderr.strip() or (gemini_res.exit_code != 0 and not gemini_has_provider_error):
        errors.append(
            {
                "command": "codexbar usage --provider gemini",
                "exitCode": gemini_res.exit_code,
                "message": safe_tail(gemini_res.stderr, max_stderr_chars) or f"Exit {gemini_res.exit_code}",
            }
        )
    for item in gemini_parsed:
        if isinstance(item, dict) and item.get("provider") != "cli":
            usage_items.append(item)

    # --- MiniMax usage from Clawdbot profiles (scan multiple directories)
    combined_clawdbot_totals = {"totalTokens": 0.0, "totalCost": 0.0, "missingCostEntries": 0.0}
    minimax_usage = None
    for state_dir in minimax_state_dirs:
        if not state_dir.is_dir():
            continue
        result = compute_clawdbot_window_usage(
            provider="minimax",
            state_dir=state_dir,
            window_minutes=minimax_session_window_minutes,
            budget_usd=minimax_session_budget_usd,
            reset_offset_minutes=minimax_session_reset_offset_minutes,
            errors=errors,
            max_stderr_chars=max_stderr_chars,
        )
        if result and isinstance(result.get("window"), dict):
            combined_clawdbot_totals["totalTokens"] += result["window"].get("totalTokens", 0.0)
            combined_clawdbot_totals["totalCost"] += result["window"].get("totalCostUSD", 0.0)
            combined_clawdbot_totals["missingCostEntries"] += result["window"].get("missingCostEntries", 0.0)
            minimax_usage = result  # Keep last result as template

    # Update minimax_usage with combined totals
    if minimax_usage and isinstance(minimax_usage.get("window"), dict):
        minimax_usage["window"]["totalTokens"] = combined_clawdbot_totals["totalTokens"]
        minimax_usage["window"]["totalCostUSD"] = combined_clawdbot_totals["totalCost"]
        minimax_usage["window"]["missingCostEntries"] = combined_clawdbot_totals["missingCostEntries"]
        if minimax_session_budget_usd > 0:
            minimax_usage["primary"]["usedPercent"] = (combined_clawdbot_totals["totalCost"] / minimax_session_budget_usd) * 100.0

    # --- MiniMax usage from direct Claude CLI: `claude --model minimax`
    direct_minimax_usage = compute_direct_claude_minimax_usage(
        window_minutes=minimax_session_window_minutes,
        budget_usd=minimax_session_budget_usd,
        reset_offset_minutes=minimax_session_reset_offset_minutes,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )

    # Combine clawdbot-minimax and direct-cli into a single minimax entry
    combined_minimax_tokens = 0.0
    combined_minimax_cost = 0.0
    combined_minimax_missing = 0.0

    if minimax_usage and isinstance(minimax_usage.get("window"), dict):
        combined_minimax_tokens += minimax_usage["window"].get("totalTokens", 0.0)
        combined_minimax_cost += minimax_usage["window"].get("totalCostUSD", 0.0)
        combined_minimax_missing += minimax_usage["window"].get("missingCostEntries", 0.0)

    if direct_minimax_usage and isinstance(direct_minimax_usage.get("window"), dict):
        combined_minimax_tokens += direct_minimax_usage["window"].get("totalTokens", 0.0)
        combined_minimax_cost += direct_minimax_usage["window"].get("totalCostUSD", 0.0)
        combined_minimax_missing += direct_minimax_usage["window"].get("missingCostEntries", 0.0)

    if minimax_usage or direct_minimax_usage:
        # Use whichever result we have as a template
        base_usage = minimax_usage or direct_minimax_usage
        combined_percent = (combined_minimax_cost / minimax_session_budget_usd * 100.0) if minimax_session_budget_usd > 0 else 0.0

        usage_items.append({
            "provider": "minimax",
            "source": "combined",
            "usage": {
                "updatedAt": base_usage.get("updatedAt"),
                "identity": base_usage.get("identity"),
                "primary": {
                    "usedPercent": combined_percent,
                    "resetsAt": base_usage.get("primary", {}).get("resetsAt"),
                    "windowMinutes": minimax_session_window_minutes,
                },
                "secondary": None,
                "tertiary": None,
                "loginMethod": base_usage.get("loginMethod"),
                "window": {
                    "budgetUSD": minimax_session_budget_usd,
                    "totalCostUSD": combined_minimax_cost,
                    "totalTokens": combined_minimax_tokens,
                    "missingCostEntries": combined_minimax_missing,
                    "resetOffsetMinutes": minimax_session_reset_offset_minutes,
                },
            },
        })
        exit_codes["usage"]["minimax"] = 0

    # Record provider-level JSON errors even if the command exited 0.
    for item in usage_items:
        if not isinstance(item, dict):
            continue
        err = item.get("error")
        if not isinstance(err, dict):
            continue
        errors.append(
            {
                "command": "codexbar usage",
                "provider": item.get("provider"),
                "profile": item.get("codexAuthAccount"),
                "exitCode": err.get("code", 0),
                "message": err.get("message", "Provider error"),
            }
        )

    # Remove any emails before publishing (public endpoint).
    remove_key_recursive(usage_items, "accountEmail")

    # --- Cost (local logs; Codex + Claude only)
    #
    # CodexBar groups daily totals by the process timezone. The dashboards need:
    # - EN: New York day boundaries
    # - RU: Minsk day boundaries
    #
    # We compute both and publish them so the frontend can switch based on language.
    def run_cost_for_lang(lang: str, tz_name: str, *, refresh: bool) -> CmdResult:
        env = dict(base_env)
        env["TZ"] = tz_name
        args = [codexbar_bin, "cost", "--provider", "both", "--format", "json", "--json-only"]
        if refresh:
            args.append("--refresh")
        return run_cmd(args, timeout_s=cost_timeout_s, env=env)

    # Refresh scan results on the first run to avoid stale "Today" totals.
    cost_res_en = run_cost_for_lang("en", "America/New_York", refresh=True)
    cost_res_ru = run_cost_for_lang("ru", "Europe/Minsk", refresh=False)

    exit_codes["cost"] = {"en": cost_res_en.exit_code, "ru": cost_res_ru.exit_code}

    if cost_res_en.stderr.strip() or cost_res_en.exit_code != 0:
        errors.append(
            {
                "command": "codexbar cost --provider both",
                "lang": "en",
                "tz": "America/New_York",
                "exitCode": cost_res_en.exit_code,
                "message": safe_tail(cost_res_en.stderr, max_stderr_chars) or f"Exit {cost_res_en.exit_code}",
            }
        )
    if cost_res_ru.stderr.strip() or cost_res_ru.exit_code != 0:
        errors.append(
            {
                "command": "codexbar cost --provider both",
                "lang": "ru",
                "tz": "Europe/Minsk",
                "exitCode": cost_res_ru.exit_code,
                "message": safe_tail(cost_res_ru.stderr, max_stderr_chars) or f"Exit {cost_res_ru.exit_code}",
            }
        )

    cost_items_en = parse_json_arrays(cost_res_en.stdout, errors=errors, context="cost.en")
    cost_items_ru = parse_json_arrays(cost_res_ru.stdout, errors=errors, context="cost.ru")
    cost_items_en = [c for c in cost_items_en if isinstance(c, dict) and c.get("provider") != "cli"]
    cost_items_ru = [c for c in cost_items_ru if isinstance(c, dict) and c.get("provider") != "cli"]

    # --- Clawdbot Claude cost (local session transcripts)
    #
    # Clawdbot uses `claude -p --output-format json ...` and records per-message
    # usage/cost into ~/.clawdbot session transcripts. CodexBar doesn't scan
    # those files, so we merge them into the Claude cost provider.
    claw_cost_en = compute_clawdbot_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="claude",
        source="clawdbot",
        match_providers=("anthropic", "claude"),
        match_model_prefixes=("claude-",),
    )
    claw_cost_ru = compute_clawdbot_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="claude",
        source="clawdbot",
        match_providers=("anthropic", "claude"),
        match_model_prefixes=("claude-",),
    )
    if claw_cost_en:
        merge_cost_items_by_provider(cost_items_en, claw_cost_en, "claude")
    if claw_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, claw_cost_ru, "claude")

    # --- Clawdbot MiniMax cost (Clawdbot profile: `clawdbot --profile minimax`)
    minimax_cost_en = compute_clawdbot_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="minimax",
        source="clawdbot-minimax",
        state_dir=minimax_state_dirs[0] if minimax_state_dirs else Path.home() / ".clawdbot-minimax",
        match_providers=("minimax",),
    )
    minimax_cost_ru = compute_clawdbot_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="minimax",
        source="clawdbot-minimax",
        state_dir=minimax_state_dirs[0] if minimax_state_dirs else Path.home() / ".clawdbot-minimax",
        match_providers=("minimax",),
    )
    if minimax_cost_en:
        merge_cost_items_by_provider(cost_items_en, minimax_cost_en, "minimax")
    if minimax_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, minimax_cost_ru, "minimax")

    # --- Direct CLI MiniMax cost (scans ~/.claude/ for direct MiniMax usage)
    #
    # Direct Claude CLI with MiniMax model stores usage in ~/.claude/ JSONL files.
    # We scan these to get cost data for the direct CLI usage.
    def compute_direct_cli_minimax_cost_delta(
        *,
        tz_name: str,
        days: int,
        errors: list[dict[str, Any]],
        max_stderr_chars: int,
    ) -> dict[str, Any] | None:
        """
        Track MiniMax cost from direct Claude CLI sessions.
        Scans ~/.claude/ directory for JSONL files with MiniMax model usage.
        """
        now_utc = datetime.now(timezone.utc)
        since_utc = now_utc - timedelta(days=max(1, days) - 1)
        since_ts_s = since_utc.timestamp()

        daily: dict[str, dict[str, Any]] = {}
        totals: dict[str, float] = {
            "inputTokens": 0.0,
            "outputTokens": 0.0,
            "cacheReadTokens": 0.0,
            "cacheCreationTokens": 0.0,
            "totalTokens": 0.0,
            "totalCost": 0.0,
            "missingCostEntries": 0.0,
        }

        # Scan ~/.claude/ directory and any custom homeDirs used by Clawdbot
        claude_dirs = [Path.home() / ".claude"]

        # Add custom homeDir used by ClaudeMinimaxBot
        minimax_custom_home = Path("/home/web/.clawdbot-claude-bypass-home-minimax/.claude")
        if minimax_custom_home.is_dir():
            claude_dirs.append(minimax_custom_home)

        # Find all JSONL files in all claude directories
        jsonl_files = []
        for claude_dir in claude_dirs:
            if not claude_dir.is_dir():
                continue
            jsonl_files.extend(claude_dir.glob("*.jsonl"))
            jsonl_files.extend(claude_dir.glob("projects/*/*.jsonl"))

        if not jsonl_files:
            return None

        for path in sorted(jsonl_files):
            try:
                st = path.stat()
            except Exception:
                continue
            if st.st_mtime < since_ts_s:
                continue

            try:
                with open(path, "r", encoding="utf-8", errors="replace") as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            entry = json.loads(line)
                        except Exception:
                            continue

                        # Check for MiniMax model in different entry formats
                        model = None
                        usage = None
                        timestamp = None

                        # Format 1: Direct message entry
                        if entry.get("type") == "assistant" and isinstance(entry.get("message"), dict):
                            message = entry.get("message", {})
                            model = message.get("model")
                            usage = message.get("usage")
                            timestamp = entry.get("timestamp")
                        # Format 2: Project session entry
                        elif isinstance(entry.get("message"), dict):
                            message = entry.get("message", {})
                            model = message.get("model")
                            usage = message.get("usage")
                            timestamp = entry.get("timestamp")

                        # Check if this is MiniMax
                        if model and "minimax" in str(model).lower():
                            if not isinstance(usage, dict):
                                continue

                            dt_utc = parse_iso_datetime(timestamp)
                            if dt_utc is None or dt_utc.timestamp() < since_ts_s:
                                continue
                            day = to_ymd_in_tz(dt_utc, tz_name)
                            if not day:
                                continue

                            bucket = daily.get(day)
                            if bucket is None:
                                bucket = {
                                    "date": day,
                                    "inputTokens": 0.0,
                                    "outputTokens": 0.0,
                                    "cacheReadTokens": 0.0,
                                    "cacheCreationTokens": 0.0,
                                    "totalTokens": 0.0,
                                    "totalCost": 0.0,
                                    "missingCostEntries": 0.0,
                                    "modelsUsed": set(),
                                    "modelCosts": {},
                                }
                                daily[day] = bucket

                            # Extract tokens (different format from Clawdbot)
                            input_tokens = as_finite_number(usage.get("input_tokens"))
                            output_tokens = as_finite_number(usage.get("output_tokens"))
                            cache_read = as_finite_number(usage.get("cache_read_input_tokens"))
                            cache_write = as_finite_number(usage.get("cache_creation_input_tokens"))
                            total_tokens = as_finite_number(usage.get("total_tokens"))

                            if total_tokens is None:
                                total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                            # Calculate cost
                            if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                                bucket["missingCostEntries"] += 1.0
                                totals["missingCostEntries"] += 1.0
                                cost_total = 0.0
                            else:
                                cost_total = compute_minimax_cost_usd(
                                    input_tokens=input_tokens,
                                    output_tokens=output_tokens,
                                    cache_read_tokens=cache_read,
                                    cache_write_tokens=cache_write,
                                )

                            bucket["inputTokens"] += input_tokens or 0.0
                            bucket["outputTokens"] += output_tokens or 0.0
                            bucket["cacheReadTokens"] += cache_read or 0.0
                            bucket["cacheCreationTokens"] += cache_write or 0.0
                            bucket["totalTokens"] += total_tokens
                            bucket["totalCost"] += cost_total

                            totals["inputTokens"] += input_tokens or 0.0
                            totals["outputTokens"] += output_tokens or 0.0
                            totals["cacheReadTokens"] += cache_read or 0.0
                            totals["cacheCreationTokens"] += cache_write or 0.0
                            totals["totalTokens"] += total_tokens
                            totals["totalCost"] += cost_total

                            if isinstance(model, str) and model:
                                bucket["modelsUsed"].add(model)
                                model_costs = bucket["modelCosts"]
                                prev_cost = model_costs.get(model, 0.0)
                                model_costs[model] = prev_cost + cost_total

            except Exception as e:
                errors.append(
                    {
                        "command": "direct claude cli cost scan",
                        "exitCode": 1,
                        "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Direct CLI logs",
                    }
                )

        if not daily:
            return None

        daily_items: list[dict[str, Any]] = []
        for day in sorted(daily.keys()):
            b = daily[day]
            model_breakdowns = [
                {"modelName": name, "cost": cost} for name, cost in sorted(b["modelCosts"].items(), key=lambda kv: kv[1], reverse=True)
            ]
            models_used = sorted(b["modelsUsed"])
            daily_items.append(
                {
                    "date": b["date"],
                    "inputTokens": b["inputTokens"],
                    "outputTokens": b["outputTokens"],
                    "cacheReadTokens": b["cacheReadTokens"],
                    "cacheCreationTokens": b["cacheCreationTokens"],
                    "totalTokens": b["totalTokens"],
                    "totalCost": b["totalCost"],
                    "modelBreakdowns": model_breakdowns,
                    "modelsUsed": models_used,
                }
                )

        return {
            "updatedAt": utc_now_iso(),
            "provider": "minimax",
            "source": "direct-cli",
            "totals": {
                "inputTokens": totals["inputTokens"],
                "outputTokens": totals["outputTokens"],
                "cacheReadTokens": totals["cacheReadTokens"],
                "cacheCreationTokens": totals["cacheCreationTokens"],
                "totalTokens": totals["totalTokens"],
                "totalCost": totals["totalCost"],
            },
            "daily": daily_items,
            "last30DaysTokens": totals["totalTokens"],
            "last30DaysCostUSD": totals["totalCost"],
        }

    direct_minimax_cost_en = compute_direct_cli_minimax_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )
    direct_minimax_cost_ru = compute_direct_cli_minimax_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )
    if direct_minimax_cost_en:
        merge_cost_items_by_provider(cost_items_en, direct_minimax_cost_en, "minimax")
    if direct_minimax_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, direct_minimax_cost_ru, "minimax")

    # Ensure the chart range always includes the last 30 calendar days,
    # even if there was no usage on some days.
    fill_missing_cost_days(cost_items_en, tz_name="America/New_York", days=30)
    fill_missing_cost_days(cost_items_ru, tz_name="Europe/Minsk", days=30)

    # Back-compat: keep the original `cost` field in EN (New York).
    cost_items = cost_items_en

    # Best-effort cleanup for rare cases where CodexBar launches a Claude CLI session
    # that detaches and keeps running.
    run_codexbar_claude_guard()

    latest = {
        "generatedAt": generated_at,
        "hostname": hostname,
        "currentCodexAccount": current_codex_account,
        "claudeActivity": claude_activity,
        "claudeRefresh": claude_refresh_meta,
        "usage": usage_items,
        "cost": cost_items,
        "costByLang": {
            "en": cost_items_en,
            "ru": cost_items_ru,
        },
        "errors": errors,
    }

    last_run = {
        "generatedAt": generated_at,
        "ok": len(errors) == 0,
        "exitCodes": exit_codes,
        "errors": errors,
    }

    atomic_write_json(data_dir / "latest.json", latest)
    atomic_write_json(data_dir / "last-run.json", last_run)
    atomic_write_json(claude_refresh_state_path, claude_refresh_state)

    # Update hourly history for heatmap (only current codex account)
    update_hourly_history(data_dir, usage_items, generated_at, current_codex_account)

    return 0 if last_run["ok"] else 0


if __name__ == "__main__":
    raise SystemExit(main())
