#!/usr/bin/env python3
import json
import os
import re
import shutil
import socket
import signal
import subprocess
import tempfile
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Any
from zoneinfo import ZoneInfo


@dataclass
class CmdResult:
    args: list[str]
    exit_code: int
    stdout: str
    stderr: str
    timeout_s: int


def utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def safe_tail(value: str, max_chars: int) -> str:
    value = value.strip()
    if len(value) <= max_chars:
        return value
    return value[-max_chars:]


def ensure_path_has_dir(env: dict[str, str], directory: str) -> dict[str, str]:
    directory = directory.strip()
    if not directory:
        return env

    current = env.get("PATH", "")
    parts = [p for p in current.split(os.pathsep) if p] if current else []
    if directory in parts:
        return env

    updated = dict(env)
    updated["PATH"] = os.pathsep.join([directory] + parts) if parts else directory
    return updated


def run_cmd(args: list[str], *, timeout_s: int, env: dict[str, str] | None = None) -> CmdResult:
    try:
        proc = subprocess.Popen(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env,
            text=True,
            encoding="utf-8",
            errors="replace",
            start_new_session=True,
        )
    except FileNotFoundError as e:
        return CmdResult(args=args, exit_code=127, stdout="", stderr=str(e), timeout_s=timeout_s)

    try:
        stdout, stderr = proc.communicate(timeout=timeout_s)
        return CmdResult(args=args, exit_code=proc.returncode or 0, stdout=stdout, stderr=stderr, timeout_s=timeout_s)
    except subprocess.TimeoutExpired as e:
        stdout = (e.output or "") if isinstance(e.output, str) else ""
        stderr = (e.stderr or "") if isinstance(e.stderr, str) else ""

        # Kill the whole process group to avoid leaving orphaned children behind.
        try:
            os.killpg(proc.pid, signal.SIGTERM)
        except ProcessLookupError:
            pass

        try:
            more_stdout, more_stderr = proc.communicate(timeout=5)
        except subprocess.TimeoutExpired:
            try:
                os.killpg(proc.pid, signal.SIGKILL)
            except ProcessLookupError:
                pass
            more_stdout, more_stderr = proc.communicate()

        if more_stdout:
            stdout = (stdout or "") + more_stdout
        if more_stderr:
            stderr = (stderr + "\n" if stderr else "") + more_stderr

        stderr = (stderr + "\n" if stderr else "") + f"Timeout after {timeout_s}s"
        return CmdResult(args=args, exit_code=124, stdout=stdout, stderr=stderr, timeout_s=timeout_s)
    except FileNotFoundError as e:
        return CmdResult(args=args, exit_code=127, stdout="", stderr=str(e), timeout_s=timeout_s)


def run_codexbar_claude_guard() -> None:
    guard_path = Path("/usr/local/sbin/codexbar-claude-guard")
    if not guard_path.is_file():
        return
    run_cmd([str(guard_path)], timeout_s=10)


def is_claude_oauth_token_expired(res: CmdResult) -> bool:
    haystack = f"{res.stdout}\n{res.stderr}".lower()
    return "claude oauth token expired" in haystack or "oauth token expired" in haystack


def resolve_claude_bin() -> str:
    """
    systemd timers often run with a minimal PATH that excludes ~/.local/bin.
    Prefer an explicit env override, then PATH lookup, then ~/.local/bin.
    """
    env_bin = os.environ.get("CLAUDE_BIN")
    if env_bin:
        return env_bin

    which_bin = shutil.which("claude")
    if which_bin:
        return which_bin

    local_bin = Path.home() / ".local" / "bin" / "claude"
    if local_bin.is_file():
        return str(local_bin)

    return "claude"


def refresh_claude_oauth_token(*, errors: list[dict[str, Any]], max_stderr_chars: int) -> None:
    """
    Best-effort auth refresh to keep CodexBar's Claude OAuth source working.

    CodexBar suggests: "Run `claude` to refresh." Running a very-low-budget
    `claude -p ...` appears to refresh tokens without making an API call.
    """
    claude_bin = resolve_claude_bin()
    max_budget_usd = os.environ.get("CLAUDE_REFRESH_MAX_BUDGET_USD", "0.0001")
    timeout_s = int(os.environ.get("CLAUDE_REFRESH_TIMEOUT_S", "60"))

    res = run_cmd(
        [claude_bin, "-p", "ping", "--output-format", "json", "--max-budget-usd", max_budget_usd],
        timeout_s=timeout_s,
    )
    if res.exit_code != 0:
        errors.append(
            {
                "command": "claude refresh",
                "exitCode": res.exit_code,
                "message": safe_tail(res.stderr, max_stderr_chars) or f"Exit {res.exit_code}",
            }
        )


def parse_json_arrays(stdout: str, *, errors: list[dict[str, Any]], context: str) -> list[Any]:
    items: list[Any] = []
    for line in stdout.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            parsed = json.loads(line)
        except Exception as e:
            errors.append(
                {
                    "command": "json.parse",
                    "context": context,
                    "exitCode": 0,
                    "message": f"Failed to parse JSON line: {e}",
                }
            )
            continue
        if isinstance(parsed, list):
            items.extend(parsed)
        else:
            items.append(parsed)
    return items


def atomic_write_json(path: Path, payload: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile("w", encoding="utf-8", dir=path.parent, prefix=f".{path.name}.", delete=False) as f:
        tmp_path = Path(f.name)
        json.dump(payload, f, ensure_ascii=False, indent=2)
        f.write("\n")
    os.chmod(tmp_path, 0o644)
    os.replace(tmp_path, path)


def load_history(path: Path) -> list[dict[str, Any]]:
    if not path.is_file():
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, list) else []
    except Exception:
        return []


def parse_iso_datetime(value: Any) -> datetime | None:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        if not isinstance(value, bool) and value > 0:
            # Heuristic: treat < 1e12 as seconds, otherwise ms.
            ts = float(value)
            if ts > 1e12:
                ts = ts / 1000.0
            try:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
            except Exception:
                return None
        return None
    if isinstance(value, str):
        s = value.strip()
        if not s:
            return None
        try:
            if s.endswith("Z"):
                s = s[:-1] + "+00:00"
            dt = datetime.fromisoformat(s)
            if dt.tzinfo is None:
                return dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        except Exception:
            return None
    return None


def to_ymd_in_tz(dt_utc: datetime, tz_name: str) -> str:
    try:
        tz = ZoneInfo(tz_name)
    except Exception:
        return ""
    try:
        return dt_utc.astimezone(tz).date().isoformat()
    except Exception:
        return ""


def as_finite_number(value: Any) -> float | None:
    if value is None:
        return None
    if isinstance(value, bool):
        return None
    if isinstance(value, (int, float)):
        n = float(value)
        return n if n >= 0 and n != float("inf") and n == n else None
    return None


def compute_clawdbot_claude_cost_delta(
    *,
    tz_name: str,
    days: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
) -> dict[str, Any] | None:
    """
    Clawdbot stores per-message usage+cost in ~/.clawdbot/agents/*/sessions/*.jsonl.
    CodexBar's `cost` scanner doesn't see these, so we merge them into the Claude
    cost provider (to match real usage when talking to Claude via Clawdbot).
    """
    state_dir = Path(os.environ.get("CLAWDBOT_STATE_DIR", str(Path.home() / ".clawdbot")))
    agents_dir = state_dir / "agents"
    if not agents_dir.is_dir():
        return None

    now_utc = datetime.now(timezone.utc)
    # Include "today" as one of the days (align with dashboard 30d windows).
    since_utc = now_utc - timedelta(days=max(1, days) - 1)
    since_ts_s = since_utc.timestamp()

    daily: dict[str, dict[str, Any]] = {}
    totals: dict[str, float] = {
        "inputTokens": 0.0,
        "outputTokens": 0.0,
        "cacheReadTokens": 0.0,
        "cacheCreationTokens": 0.0,
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    session_files = sorted(agents_dir.glob("*/sessions/*.jsonl"))
    for path in session_files:
        if path.name.endswith(".jsonl.lock"):
            continue
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < since_ts_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue
                    if not isinstance(entry, dict):
                        continue
                    if entry.get("type") != "message":
                        continue
                    msg = entry.get("message")
                    if not isinstance(msg, dict):
                        continue
                    if msg.get("role") != "assistant":
                        continue

                    provider = msg.get("provider") or entry.get("provider")
                    model = msg.get("model") or entry.get("model")
                    # Only merge Anthropic/Claude usage; ignore other providers.
                    if provider not in ("anthropic", "claude") and not str(model or "").startswith("claude-"):
                        continue

                    usage = msg.get("usage") or entry.get("usage")
                    if not isinstance(usage, dict):
                        continue

                    dt_utc = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
                    if dt_utc is None or dt_utc.timestamp() < since_ts_s:
                        continue
                    day = to_ymd_in_tz(dt_utc, tz_name)
                    if not day:
                        continue

                    bucket = daily.get(day)
                    if bucket is None:
                        bucket = {
                            "date": day,
                            "inputTokens": 0.0,
                            "outputTokens": 0.0,
                            "cacheReadTokens": 0.0,
                            "cacheCreationTokens": 0.0,
                            "totalTokens": 0.0,
                            "totalCost": 0.0,
                            "missingCostEntries": 0.0,
                            "modelsUsed": set(),
                            "modelCosts": {},
                        }
                        daily[day] = bucket

                    input_tokens = as_finite_number(usage.get("input"))
                    output_tokens = as_finite_number(usage.get("output"))
                    cache_read = as_finite_number(usage.get("cacheRead"))
                    cache_write = as_finite_number(usage.get("cacheWrite"))
                    total_tokens = as_finite_number(usage.get("totalTokens")) or as_finite_number(usage.get("total"))
                    if total_tokens is None:
                        total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                    cost_total = None
                    cost = usage.get("cost")
                    if isinstance(cost, dict):
                        cost_total = as_finite_number(cost.get("total"))
                    if cost_total is None:
                        bucket["missingCostEntries"] += 1.0
                        totals["missingCostEntries"] += 1.0
                        cost_total = 0.0

                    bucket["inputTokens"] += input_tokens or 0.0
                    bucket["outputTokens"] += output_tokens or 0.0
                    bucket["cacheReadTokens"] += cache_read or 0.0
                    bucket["cacheCreationTokens"] += cache_write or 0.0
                    bucket["totalTokens"] += total_tokens
                    bucket["totalCost"] += cost_total

                    totals["inputTokens"] += input_tokens or 0.0
                    totals["outputTokens"] += output_tokens or 0.0
                    totals["cacheReadTokens"] += cache_read or 0.0
                    totals["cacheCreationTokens"] += cache_write or 0.0
                    totals["totalTokens"] += total_tokens
                    totals["totalCost"] += cost_total

                    if isinstance(model, str) and model:
                        bucket["modelsUsed"].add(model)
                        model_costs = bucket["modelCosts"]
                        prev_cost = model_costs.get(model, 0.0)
                        model_costs[model] = prev_cost + cost_total
        except Exception as e:
            errors.append(
                {
                    "command": "clawdbot transcripts scan",
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Clawdbot sessions",
                }
            )

    if not daily:
        return None

    daily_items: list[dict[str, Any]] = []
    for day in sorted(daily.keys()):
        b = daily[day]
        model_breakdowns = [
            {"modelName": name, "cost": cost} for name, cost in sorted(b["modelCosts"].items(), key=lambda kv: kv[1], reverse=True)
        ]
        models_used = sorted(b["modelsUsed"])
        daily_items.append(
            {
                "date": b["date"],
                "inputTokens": b["inputTokens"],
                "outputTokens": b["outputTokens"],
                "cacheReadTokens": b["cacheReadTokens"],
                "cacheCreationTokens": b["cacheCreationTokens"],
                "totalTokens": b["totalTokens"],
                "totalCost": b["totalCost"],
                "modelBreakdowns": model_breakdowns,
                "modelsUsed": models_used,
            }
        )

    return {
        "updatedAt": utc_now_iso(),
        "provider": "claude",
        "source": "clawdbot",
        "totals": {
            "inputTokens": totals["inputTokens"],
            "outputTokens": totals["outputTokens"],
            "cacheReadTokens": totals["cacheReadTokens"],
            "cacheCreationTokens": totals["cacheCreationTokens"],
            "totalTokens": totals["totalTokens"],
            "totalCost": totals["totalCost"],
        },
        "daily": daily_items,
        "last30DaysTokens": totals["totalTokens"],
        "last30DaysCostUSD": totals["totalCost"],
    }


def merge_cost_items_by_provider(base: list[Any], delta: dict[str, Any], provider: str) -> None:
    if not delta:
        return

    target = None
    for item in base:
        if isinstance(item, dict) and item.get("provider") == provider:
            target = item
            break

    if target is None:
        base.append(delta)
        return

    # Merge totals + 30d totals
    target_totals = target.get("totals")
    if not isinstance(target_totals, dict):
        target_totals = {}
        target["totals"] = target_totals
    delta_totals = delta.get("totals")
    if isinstance(delta_totals, dict):
        for key in ["inputTokens", "outputTokens", "cacheReadTokens", "cacheCreationTokens", "totalTokens", "totalCost"]:
            a = as_finite_number(target_totals.get(key)) or 0.0
            b = as_finite_number(delta_totals.get(key)) or 0.0
            target_totals[key] = a + b

    for key in ["last30DaysTokens", "last30DaysCostUSD"]:
        a = as_finite_number(target.get(key)) or 0.0
        b = as_finite_number(delta.get(key)) or 0.0
        target[key] = a + b

    # Mark combined source to avoid confusion.
    base_source = str(target.get("source") or "").strip() or "local"
    delta_source = str(delta.get("source") or "").strip()
    if delta_source and delta_source not in base_source:
        target["source"] = f"{base_source}+{delta_source}"

    # Merge daily entries by date
    target_daily = target.get("daily")
    if not isinstance(target_daily, list):
        target_daily = []
        target["daily"] = target_daily
    by_date: dict[str, dict[str, Any]] = {}
    for d in target_daily:
        if isinstance(d, dict) and isinstance(d.get("date"), str):
            by_date[d["date"]] = d

    delta_daily = delta.get("daily")
    if isinstance(delta_daily, list):
        for d in delta_daily:
            if not isinstance(d, dict) or not isinstance(d.get("date"), str):
                continue
            date = d["date"]
            existing = by_date.get(date)
            if existing is None:
                target_daily.append(d)
                by_date[date] = d
                continue

            for key in ["inputTokens", "outputTokens", "cacheReadTokens", "cacheCreationTokens", "totalTokens", "totalCost"]:
                a = as_finite_number(existing.get(key)) or 0.0
                b = as_finite_number(d.get(key)) or 0.0
                existing[key] = a + b

            # Merge modelsUsed
            models_used = set(existing.get("modelsUsed") or []) if isinstance(existing.get("modelsUsed"), list) else set()
            models_used.update(d.get("modelsUsed") or [])
            existing["modelsUsed"] = sorted([m for m in models_used if isinstance(m, str) and m])

            # Merge modelBreakdowns
            def to_cost_map(items: Any) -> dict[str, float]:
                out: dict[str, float] = {}
                if not isinstance(items, list):
                    return out
                for mb in items:
                    if not isinstance(mb, dict):
                        continue
                    name = mb.get("modelName")
                    cost = as_finite_number(mb.get("cost")) or 0.0
                    if isinstance(name, str) and name:
                        out[name] = out.get(name, 0.0) + cost
                return out

            merged = to_cost_map(existing.get("modelBreakdowns"))
            for name, cost in to_cost_map(d.get("modelBreakdowns")).items():
                merged[name] = merged.get(name, 0.0) + cost
            existing["modelBreakdowns"] = [
                {"modelName": name, "cost": cost} for name, cost in sorted(merged.items(), key=lambda kv: kv[1], reverse=True)
            ]

    target_daily.sort(key=lambda d: d.get("date") or "")


def update_hourly_history(data_dir: Path, usage_items: list[dict[str, Any]], generated_at: str, current_codex_account: str | None = None) -> None:
    """
    Track hourly usage history for heatmap visualization.
    Calculates delta (activity) by comparing current values to previous run.
    Each entry: {ts, hour, day, provider, account, activity}
    Keep last 30 days of data.
    """
    history_path = data_dir / "history.json"
    state_path = data_dir / ".history_state.json"

    history = load_history(history_path)
    prev_state = load_history(state_path) if state_path.is_file() else {}
    if isinstance(prev_state, list):
        prev_state = {}

    # Use local time for hour/day display
    now_local = datetime.now()
    now = datetime.now(timezone.utc)
    hour = now_local.hour
    day = now_local.strftime("%Y-%m-%d")

    current_state: dict[str, Any] = {}

    # Extract usage percentages and calculate deltas
    for item in usage_items:
        if not isinstance(item, dict):
            continue

        provider = item.get("provider", "unknown")
        account = item.get("codexAuthAccount", "")

        usage = item.get("usage", {})
        if not isinstance(usage, dict):
            continue

        primary = usage.get("primary", {})
        session_pct = primary.get("usedPercent") if isinstance(primary, dict) else None

        if session_pct is None:
            continue

        key = f"{provider}|{account}"
        # Always update state for all accounts (needed for accurate delta calculation on switch)
        current_state[key] = {"sessionPct": session_pct, "ts": generated_at}

        # Calculate activity delta
        prev = prev_state.get(key, {})
        prev_pct = prev.get("sessionPct")

        activity = 0.0
        if prev_pct is not None and session_pct is not None:
            # If session reset (current < previous), count current as new activity
            if session_pct < prev_pct - 0.5:  # Allow small tolerance for reset detection
                activity = session_pct
            else:
                activity = session_pct - prev_pct
        elif session_pct is not None and session_pct > 0:
            # First run or no previous data - if there's usage, record it
            activity = session_pct

        # For codex provider, only record history for the current active account
        if provider == "codex" and current_codex_account and account != current_codex_account:
            continue

        # Only record if there was meaningful activity (> 0.1% to avoid noise)
        if activity > 0.1:
            entry = {
                "ts": generated_at,
                "hour": hour,
                "day": day,
                "provider": provider,
                "account": account,
                "activity": round(activity, 1),
            }
            history.append(entry)

    # Save current state for next comparison
    atomic_write_json(state_path, current_state)

    # Prune entries older than 30 days
    cutoff = (now - timedelta(days=30)).strftime("%Y-%m-%d")
    history = [e for e in history if e.get("day", "") >= cutoff]

    atomic_write_json(history_path, history)


def parse_accounts_env(value: str) -> list[str]:
    value = value.strip()
    if not value:
        return []
    return [part for part in re.split(r"[,\s]+", value) if part]


def remove_key_recursive(obj: Any, key: str) -> None:
    if isinstance(obj, dict):
        if key in obj:
            obj.pop(key, None)
        for v in list(obj.values()):
            remove_key_recursive(v, key)
    elif isinstance(obj, list):
        for v in obj:
            remove_key_recursive(v, key)


def main() -> int:
    os.umask(0o022)

    base_dir = Path(__file__).resolve().parent.parent
    data_dir = base_dir / "public" / "data"

    base_env = ensure_path_has_dir(dict(os.environ), str(Path.home() / ".local" / "bin"))

    codexbar_bin = os.environ.get("CODEXBAR_BIN", "/usr/local/bin/codexbar")
    usage_timeout_s = int(os.environ.get("USAGE_TIMEOUT_S", "60"))
    cost_timeout_s = int(os.environ.get("COST_TIMEOUT_S", "180"))
    max_stderr_chars = int(os.environ.get("MAX_STDERR_CHARS", "4000"))

    codex_accounts_dir = Path(os.environ.get("CODEX_ACCOUNTS_DIR", str(Path.home() / ".codex" / "accounts")))
    codex_config_toml = Path(os.environ.get("CODEX_CONFIG_TOML", str(Path.home() / ".codex" / "config.toml")))
    selected_accounts = parse_accounts_env(os.environ.get("CODEX_AUTH_ACCOUNTS", ""))

    generated_at = utc_now_iso()
    hostname = socket.gethostname()

    # Get current active codex account
    current_codex_account = None
    current_file = Path.home() / ".codex" / "current"
    if current_file.is_file():
        try:
            current_codex_account = current_file.read_text().strip()
        except Exception:
            pass

    errors: list[dict[str, Any]] = []
    usage_items: list[dict[str, Any]] = []
    cost_items: list[Any] = []

    exit_codes: dict[str, Any] = {"usage": {"codex": {}, "claude": None, "gemini": None}, "cost": None}

    # --- Codex: multi-account via codex-auth profiles (isolated HOME per profile; no global switching).
    all_account_files = sorted(codex_accounts_dir.glob("*.json")) if codex_accounts_dir.is_dir() else []
    if selected_accounts:
        by_name = {p.stem: p for p in all_account_files}
        account_files: list[Path] = []
        for name in selected_accounts:
            p = by_name.get(name)
            if p is None:
                errors.append(
                    {
                        "command": "codexbar usage --provider codex",
                        "profile": name,
                        "exitCode": 1,
                        "message": f"codex-auth profile not found: {codex_accounts_dir}/{name}.json",
                    }
                )
                continue
            account_files.append(p)
    else:
        account_files = all_account_files

    if not account_files:
        errors.append(
            {
                "command": "codexbar usage --provider codex",
                "exitCode": 1,
                "message": f"No codex-auth accounts found in {codex_accounts_dir}",
            }
        )

    for account_file in account_files:
        profile = account_file.stem
        with tempfile.TemporaryDirectory(prefix="codex-home-") as tmp_home:
            tmp_home_path = Path(tmp_home)
            tmp_codex_dir = tmp_home_path / ".codex"
            tmp_codex_dir.mkdir(parents=True, exist_ok=True)
            (tmp_codex_dir / "auth.json").symlink_to(account_file)
            if codex_config_toml.is_file():
                shutil.copy2(codex_config_toml, tmp_codex_dir / "config.toml")

            env = dict(base_env)
            env["HOME"] = tmp_home

            res = run_cmd(
                [codexbar_bin, "usage", "--provider", "codex", "--format", "json", "--json-only"],
                timeout_s=usage_timeout_s,
                env=env,
            )

        exit_codes["usage"]["codex"][profile] = res.exit_code

        if res.stderr.strip() or res.exit_code != 0:
            errors.append(
                {
                    "command": "codexbar usage --provider codex",
                    "profile": profile,
                    "exitCode": res.exit_code,
                    "message": safe_tail(res.stderr, max_stderr_chars) or f"Exit {res.exit_code}",
                }
            )

        parsed = parse_json_arrays(res.stdout, errors=errors, context=f"usage.codex.{profile}")
        for item in parsed:
            if isinstance(item, dict):
                item["codexAuthAccount"] = profile
                usage_items.append(item)

    # --- Claude usage (OAuth, configured in ~/.codexbar/config.json)
    claude_res = run_cmd(
        [codexbar_bin, "usage", "--provider", "claude", "--format", "json", "--json-only"],
        timeout_s=usage_timeout_s,
        env=base_env,
    )
    if is_claude_oauth_token_expired(claude_res):
        refresh_claude_oauth_token(errors=errors, max_stderr_chars=max_stderr_chars)
        claude_res = run_cmd(
            [codexbar_bin, "usage", "--provider", "claude", "--format", "json", "--json-only"],
            timeout_s=usage_timeout_s,
            env=base_env,
        )
    exit_codes["usage"]["claude"] = claude_res.exit_code
    if claude_res.stderr.strip() or claude_res.exit_code != 0:
        errors.append(
            {
                "command": "codexbar usage --provider claude",
                "exitCode": claude_res.exit_code,
                "message": safe_tail(claude_res.stderr, max_stderr_chars) or f"Exit {claude_res.exit_code}",
            }
        )
    for item in parse_json_arrays(claude_res.stdout, errors=errors, context="usage.claude"):
        if isinstance(item, dict):
            usage_items.append(item)

    # --- Gemini usage (API via Gemini CLI credentials)
    gemini_res = run_cmd(
        [codexbar_bin, "usage", "--provider", "gemini", "--source", "api", "--format", "json", "--json-only"],
        timeout_s=usage_timeout_s,
        env=base_env,
    )
    exit_codes["usage"]["gemini"] = gemini_res.exit_code
    if gemini_res.stderr.strip() or gemini_res.exit_code != 0:
        errors.append(
            {
                "command": "codexbar usage --provider gemini",
                "exitCode": gemini_res.exit_code,
                "message": safe_tail(gemini_res.stderr, max_stderr_chars) or f"Exit {gemini_res.exit_code}",
            }
        )
    for item in parse_json_arrays(gemini_res.stdout, errors=errors, context="usage.gemini"):
        if isinstance(item, dict):
            usage_items.append(item)

    # Record provider-level JSON errors even if the command exited 0.
    for item in usage_items:
        if not isinstance(item, dict):
            continue
        err = item.get("error")
        if not isinstance(err, dict):
            continue
        errors.append(
            {
                "command": "codexbar usage",
                "provider": item.get("provider"),
                "profile": item.get("codexAuthAccount"),
                "exitCode": err.get("code", 0),
                "message": err.get("message", "Provider error"),
            }
        )

    # Remove any emails before publishing (public endpoint).
    remove_key_recursive(usage_items, "accountEmail")

    # --- Cost (local logs; Codex + Claude only)
    #
    # CodexBar groups daily totals by the process timezone. The dashboards need:
    # - EN: New York day boundaries
    # - RU: Minsk day boundaries
    #
    # We compute both and publish them so the frontend can switch based on language.
    def run_cost_for_lang(lang: str, tz_name: str, *, refresh: bool) -> CmdResult:
        env = dict(base_env)
        env["TZ"] = tz_name
        args = [codexbar_bin, "cost", "--provider", "both", "--format", "json", "--json-only"]
        if refresh:
            args.append("--refresh")
        return run_cmd(args, timeout_s=cost_timeout_s, env=env)

    # Refresh scan results on the first run to avoid stale "Today" totals.
    cost_res_en = run_cost_for_lang("en", "America/New_York", refresh=True)
    cost_res_ru = run_cost_for_lang("ru", "Europe/Minsk", refresh=False)

    exit_codes["cost"] = {"en": cost_res_en.exit_code, "ru": cost_res_ru.exit_code}

    if cost_res_en.stderr.strip() or cost_res_en.exit_code != 0:
        errors.append(
            {
                "command": "codexbar cost --provider both",
                "lang": "en",
                "tz": "America/New_York",
                "exitCode": cost_res_en.exit_code,
                "message": safe_tail(cost_res_en.stderr, max_stderr_chars) or f"Exit {cost_res_en.exit_code}",
            }
        )
    if cost_res_ru.stderr.strip() or cost_res_ru.exit_code != 0:
        errors.append(
            {
                "command": "codexbar cost --provider both",
                "lang": "ru",
                "tz": "Europe/Minsk",
                "exitCode": cost_res_ru.exit_code,
                "message": safe_tail(cost_res_ru.stderr, max_stderr_chars) or f"Exit {cost_res_ru.exit_code}",
            }
        )

    cost_items_en = parse_json_arrays(cost_res_en.stdout, errors=errors, context="cost.en")
    cost_items_ru = parse_json_arrays(cost_res_ru.stdout, errors=errors, context="cost.ru")

    # --- Clawdbot Claude cost (local session transcripts)
    #
    # Clawdbot uses `claude -p --output-format json ...` and records per-message
    # usage/cost into ~/.clawdbot session transcripts. CodexBar doesn't scan
    # those files, so we merge them into the Claude cost provider.
    claw_cost_en = compute_clawdbot_claude_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )
    claw_cost_ru = compute_clawdbot_claude_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )
    if claw_cost_en:
        merge_cost_items_by_provider(cost_items_en, claw_cost_en, "claude")
    if claw_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, claw_cost_ru, "claude")

    # Back-compat: keep the original `cost` field in EN (New York).
    cost_items = cost_items_en

    # Best-effort cleanup for rare cases where CodexBar launches a Claude CLI session
    # that detaches and keeps running.
    run_codexbar_claude_guard()

    latest = {
        "generatedAt": generated_at,
        "hostname": hostname,
        "currentCodexAccount": current_codex_account,
        "usage": usage_items,
        "cost": cost_items,
        "costByLang": {
            "en": cost_items_en,
            "ru": cost_items_ru,
        },
        "errors": errors,
    }

    last_run = {
        "generatedAt": generated_at,
        "ok": len(errors) == 0,
        "exitCodes": exit_codes,
        "errors": errors,
    }

    atomic_write_json(data_dir / "latest.json", latest)
    atomic_write_json(data_dir / "last-run.json", last_run)

    # Update hourly history for heatmap (only current codex account)
    update_hourly_history(data_dir, usage_items, generated_at, current_codex_account)

    return 0 if last_run["ok"] else 0


if __name__ == "__main__":
    raise SystemExit(main())
