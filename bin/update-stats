#!/usr/bin/env python3
import json
import os
import re
import shutil
import socket
import signal
import subprocess
import tempfile
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any
from zoneinfo import ZoneInfo


@dataclass
class CmdResult:
    args: list[str]
    exit_code: int
    stdout: str
    stderr: str
    timeout_s: int


def utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def safe_tail(value: str, max_chars: int) -> str:
    value = value.strip()
    if len(value) <= max_chars:
        return value
    return value[-max_chars:]


def ensure_path_has_dir(env: dict[str, str], directory: str) -> dict[str, str]:
    directory = directory.strip()
    if not directory:
        return env

    current = env.get("PATH", "")
    parts = [p for p in current.split(os.pathsep) if p] if current else []
    if directory in parts:
        return env

    updated = dict(env)
    updated["PATH"] = os.pathsep.join([directory] + parts) if parts else directory
    return updated


def run_cmd(args: list[str], *, timeout_s: int, env: dict[str, str] | None = None) -> CmdResult:
    try:
        proc = subprocess.Popen(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env,
            text=True,
            encoding="utf-8",
            errors="replace",
            start_new_session=True,
        )
    except FileNotFoundError as e:
        return CmdResult(args=args, exit_code=127, stdout="", stderr=str(e), timeout_s=timeout_s)

    try:
        stdout, stderr = proc.communicate(timeout=timeout_s)
        return CmdResult(args=args, exit_code=proc.returncode or 0, stdout=stdout, stderr=stderr, timeout_s=timeout_s)
    except subprocess.TimeoutExpired as e:
        stdout = (e.output or "") if isinstance(e.output, str) else ""
        stderr = (e.stderr or "") if isinstance(e.stderr, str) else ""

        # Kill the whole process group to avoid leaving orphaned children behind.
        try:
            os.killpg(proc.pid, signal.SIGTERM)
        except ProcessLookupError:
            pass

        try:
            more_stdout, more_stderr = proc.communicate(timeout=5)
        except subprocess.TimeoutExpired:
            try:
                os.killpg(proc.pid, signal.SIGKILL)
            except ProcessLookupError:
                pass
            more_stdout, more_stderr = proc.communicate()

        if more_stdout:
            stdout = (stdout or "") + more_stdout
        if more_stderr:
            stderr = (stderr + "\n" if stderr else "") + more_stderr

        stderr = (stderr + "\n" if stderr else "") + f"Timeout after {timeout_s}s"
        return CmdResult(args=args, exit_code=124, stdout=stdout, stderr=stderr, timeout_s=timeout_s)
    except FileNotFoundError as e:
        return CmdResult(args=args, exit_code=127, stdout="", stderr=str(e), timeout_s=timeout_s)


def run_codexbar_claude_guard() -> None:
    guard_path = Path("/usr/local/sbin/codexbar-claude-guard")
    if not guard_path.is_file():
        return
    run_cmd([str(guard_path)], timeout_s=10)


def is_claude_oauth_token_expired(res: CmdResult) -> bool:
    haystack = f"{res.stdout}\n{res.stderr}".lower()
    return "claude oauth token expired" in haystack or "oauth token expired" in haystack


def resolve_claude_bin() -> str:
    """
    systemd timers often run with a minimal PATH that excludes ~/.local/bin.
    Prefer an explicit env override, then PATH lookup, then ~/.local/bin.
    """
    env_bin = os.environ.get("CLAUDE_BIN")
    if env_bin:
        return env_bin

    which_bin = shutil.which("claude")
    if which_bin:
        return which_bin

    local_bin = Path.home() / ".local" / "bin" / "claude"
    if local_bin.is_file():
        return str(local_bin)

    return "claude"


def refresh_claude_oauth_token(*, errors: list[dict[str, Any]], max_stderr_chars: int) -> None:
    """
    Best-effort auth refresh to keep CodexBar's Claude OAuth source working.

    CodexBar suggests: "Run `claude` to refresh." Running a very-low-budget
    `claude -p ...` appears to refresh tokens without making an API call.
    """
    claude_bin = resolve_claude_bin()
    max_budget_usd = os.environ.get("CLAUDE_REFRESH_MAX_BUDGET_USD", "0.0001")
    timeout_s = int(os.environ.get("CLAUDE_REFRESH_TIMEOUT_S", "60"))

    res = run_cmd(
        [claude_bin, "-p", "ping", "--output-format", "json", "--max-budget-usd", max_budget_usd],
        timeout_s=timeout_s,
    )
    if res.exit_code != 0:
        msg = safe_tail(res.stderr, max_stderr_chars).strip()
        if not msg:
            # Claude Code emits structured errors on stdout in JSON mode.
            last_line = ""
            for line in res.stdout.splitlines()[::-1]:
                if line.strip():
                    last_line = line.strip()
                    break
            if last_line:
                try:
                    parsed = json.loads(last_line)
                except Exception:
                    parsed = None
                if isinstance(parsed, dict) and isinstance(parsed.get("result"), str):
                    msg = safe_tail(parsed["result"], max_stderr_chars).strip()
            if not msg:
                msg = safe_tail(res.stdout, max_stderr_chars).strip()

        if "oauth token" in msg.lower() or "authentication_error" in msg.lower():
            msg = f"{msg} (Fix: run `claude setup-token` or open `claude` and re-auth.)"

        errors.append(
            {
                "command": "claude refresh",
                "exitCode": res.exit_code,
                "message": msg or f"Exit {res.exit_code}",
            }
        )


def parse_json_arrays(stdout: str, *, errors: list[dict[str, Any]], context: str) -> list[Any]:
    items: list[Any] = []
    for line in stdout.splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            parsed = json.loads(line)
        except Exception as e:
            errors.append(
                {
                    "command": "json.parse",
                    "context": context,
                    "exitCode": 0,
                    "message": f"Failed to parse JSON line: {e}",
                }
            )
            continue
        if isinstance(parsed, list):
            items.extend(parsed)
        else:
            items.append(parsed)
    return items


def atomic_write_json(path: Path, payload: Any) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile("w", encoding="utf-8", dir=path.parent, prefix=f".{path.name}.", delete=False) as f:
        tmp_path = Path(f.name)
        json.dump(payload, f, ensure_ascii=False, indent=2)
        f.write("\n")
    os.chmod(tmp_path, 0o644)
    os.replace(tmp_path, path)


def load_history(path: Path) -> list[dict[str, Any]]:
    if not path.is_file():
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data if isinstance(data, list) else []
    except Exception:
        return []


def parse_iso_datetime(value: Any) -> datetime | None:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        if not isinstance(value, bool) and value > 0:
            # Heuristic: treat < 1e12 as seconds, otherwise ms.
            ts = float(value)
            if ts > 1e12:
                ts = ts / 1000.0
            try:
                return datetime.fromtimestamp(ts, tz=timezone.utc)
            except Exception:
                return None
        return None
    if isinstance(value, str):
        s = value.strip()
        if not s:
            return None
        try:
            if s.endswith("Z"):
                s = s[:-1] + "+00:00"
            dt = datetime.fromisoformat(s)
            if dt.tzinfo is None:
                return dt.replace(tzinfo=timezone.utc)
            return dt.astimezone(timezone.utc)
        except Exception:
            return None
    return None


def to_ymd_in_tz(dt_utc: datetime, tz_name: str) -> str:
    try:
        tz = ZoneInfo(tz_name)
    except Exception:
        return ""
    try:
        return dt_utc.astimezone(tz).date().isoformat()
    except Exception:
        return ""


def as_finite_number(value: Any) -> float | None:
    if value is None:
        return None
    if isinstance(value, bool):
        return None
    if isinstance(value, (int, float)):
        n = float(value)
        return n if n >= 0 and n != float("inf") and n == n else None
    return None


@lru_cache(maxsize=1)
def minimax_pricing_usd_per_million_tokens() -> dict[str, float]:
    def env_float(name: str, default: float) -> float:
        value = os.environ.get(name)
        if value is None:
            return default
        try:
            parsed = float(value)
        except Exception:
            return default
        return parsed if parsed >= 0 and parsed == parsed and parsed != float("inf") else default

    return {
        "input": env_float("MINIMAX_COST_INPUT_PER_MILLION_USD", 0.3),
        "output": env_float("MINIMAX_COST_OUTPUT_PER_MILLION_USD", 1.2),
        "cacheRead": env_float("MINIMAX_COST_CACHE_READ_PER_MILLION_USD", 0.03),
        "cacheWrite": env_float("MINIMAX_COST_CACHE_WRITE_PER_MILLION_USD", 0.375),
    }


def compute_minimax_cost_usd(
    *,
    input_tokens: float | None,
    output_tokens: float | None,
    cache_read_tokens: float | None,
    cache_write_tokens: float | None,
) -> float:
    pricing = minimax_pricing_usd_per_million_tokens()
    return (
        (input_tokens or 0.0) * pricing["input"]
        + (output_tokens or 0.0) * pricing["output"]
        + (cache_read_tokens or 0.0) * pricing["cacheRead"]
        + (cache_write_tokens or 0.0) * pricing["cacheWrite"]
    ) / 1_000_000.0


def compute_clawdbot_cost_delta(
    *,
    tz_name: str,
    days: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
    provider: str,
    source: str,
    state_dir: Path | None = None,
    match_providers: tuple[str, ...] | None = None,
    match_model_prefixes: tuple[str, ...] | None = None,
) -> dict[str, Any] | None:
    """
    Clawdbot stores per-message usage+cost in ~/.clawdbot*/agents/*/sessions/*.jsonl.
    CodexBar's `cost` scanner doesn't see these, so we merge them into the
    relevant cost provider.
    """
    if state_dir is None:
        state_dir = Path(os.environ.get("CLAWDBOT_STATE_DIR", str(Path.home() / ".clawdbot")))

    provider_out = str(provider or "").strip() or "unknown"
    source_out = str(source or "").strip() or "clawdbot"
    provider_match = tuple(match_providers) if match_providers is not None else (provider_out,)
    provider_match_lower = tuple(str(p).strip().lower() for p in provider_match if str(p).strip())
    model_prefixes = tuple(match_model_prefixes) if match_model_prefixes is not None else tuple()

    agents_dir = state_dir / "agents"
    if not agents_dir.is_dir():
        return None

    now_utc = datetime.now(timezone.utc)
    # Include "today" as one of the days (align with dashboard 30d windows).
    since_utc = now_utc - timedelta(days=max(1, days) - 1)
    since_ts_s = since_utc.timestamp()

    daily: dict[str, dict[str, Any]] = {}
    totals: dict[str, float] = {
        "inputTokens": 0.0,
        "outputTokens": 0.0,
        "cacheReadTokens": 0.0,
        "cacheCreationTokens": 0.0,
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    session_files = sorted(agents_dir.glob("*/sessions/*.jsonl"))
    for path in session_files:
        if path.name.endswith(".jsonl.lock"):
            continue
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < since_ts_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue
                    if not isinstance(entry, dict):
                        continue
                    if entry.get("type") != "message":
                        continue
                    msg = entry.get("message")
                    if not isinstance(msg, dict):
                        continue
                    if msg.get("role") != "assistant":
                        continue

                    provider = msg.get("provider") or entry.get("provider")
                    model = msg.get("model") or entry.get("model")
                    provider_value = str(provider or "").strip().lower()
                    model_value = str(model or "")
                    if provider_match_lower and provider_value not in provider_match_lower and not any(
                        model_value.startswith(prefix) for prefix in model_prefixes
                    ):
                        continue

                    usage = msg.get("usage") or entry.get("usage")
                    if not isinstance(usage, dict):
                        continue

                    dt_utc = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
                    if dt_utc is None or dt_utc.timestamp() < since_ts_s:
                        continue
                    day = to_ymd_in_tz(dt_utc, tz_name)
                    if not day:
                        continue

                    bucket = daily.get(day)
                    if bucket is None:
                        bucket = {
                            "date": day,
                            "inputTokens": 0.0,
                            "outputTokens": 0.0,
                            "cacheReadTokens": 0.0,
                            "cacheCreationTokens": 0.0,
                            "totalTokens": 0.0,
                            "totalCost": 0.0,
                            "missingCostEntries": 0.0,
                            "modelsUsed": set(),
                            "modelCosts": {},
                        }
                        daily[day] = bucket

                    input_tokens = as_finite_number(usage.get("input"))
                    output_tokens = as_finite_number(usage.get("output"))
                    cache_read = as_finite_number(usage.get("cacheRead"))
                    cache_write = as_finite_number(usage.get("cacheWrite"))
                    total_tokens = as_finite_number(usage.get("totalTokens")) or as_finite_number(usage.get("total"))
                    if total_tokens is None:
                        total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                    cost_total = None
                    if provider_value == "minimax":
                        if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                            bucket["missingCostEntries"] += 1.0
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0
                        else:
                            cost_total = compute_minimax_cost_usd(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                cache_read_tokens=cache_read,
                                cache_write_tokens=cache_write,
                            )
                    else:
                        cost = usage.get("cost")
                        if isinstance(cost, dict):
                            cost_total = as_finite_number(cost.get("total"))
                        if cost_total is None:
                            bucket["missingCostEntries"] += 1.0
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0

                    bucket["inputTokens"] += input_tokens or 0.0
                    bucket["outputTokens"] += output_tokens or 0.0
                    bucket["cacheReadTokens"] += cache_read or 0.0
                    bucket["cacheCreationTokens"] += cache_write or 0.0
                    bucket["totalTokens"] += total_tokens
                    bucket["totalCost"] += cost_total

                    totals["inputTokens"] += input_tokens or 0.0
                    totals["outputTokens"] += output_tokens or 0.0
                    totals["cacheReadTokens"] += cache_read or 0.0
                    totals["cacheCreationTokens"] += cache_write or 0.0
                    totals["totalTokens"] += total_tokens
                    totals["totalCost"] += cost_total

                    if isinstance(model, str) and model:
                        bucket["modelsUsed"].add(model)
                        model_costs = bucket["modelCosts"]
                        prev_cost = model_costs.get(model, 0.0)
                        model_costs[model] = prev_cost + cost_total
        except Exception as e:
            errors.append(
                {
                    "command": "clawdbot transcripts scan",
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Clawdbot sessions",
                }
            )

    if not daily:
        return None

    daily_items: list[dict[str, Any]] = []
    for day in sorted(daily.keys()):
        b = daily[day]
        model_breakdowns = [
            {"modelName": name, "cost": cost} for name, cost in sorted(b["modelCosts"].items(), key=lambda kv: kv[1], reverse=True)
        ]
        models_used = sorted(b["modelsUsed"])
        daily_items.append(
            {
                "date": b["date"],
                "inputTokens": b["inputTokens"],
                "outputTokens": b["outputTokens"],
                "cacheReadTokens": b["cacheReadTokens"],
                "cacheCreationTokens": b["cacheCreationTokens"],
                "totalTokens": b["totalTokens"],
                "totalCost": b["totalCost"],
                "modelBreakdowns": model_breakdowns,
                "modelsUsed": models_used,
            }
            )

    return {
        "updatedAt": utc_now_iso(),
        "provider": provider_out,
        "source": source_out,
        "totals": {
            "inputTokens": totals["inputTokens"],
            "outputTokens": totals["outputTokens"],
            "cacheReadTokens": totals["cacheReadTokens"],
            "cacheCreationTokens": totals["cacheCreationTokens"],
            "totalTokens": totals["totalTokens"],
            "totalCost": totals["totalCost"],
        },
        "daily": daily_items,
        "last30DaysTokens": totals["totalTokens"],
        "last30DaysCostUSD": totals["totalCost"],
    }


def compute_clawdbot_window_usage(
    *,
    provider: str,
    state_dir: Path,
    window_minutes: int,
    budget_usd: float,
    reset_offset_minutes: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
) -> dict[str, Any] | None:
    """
    Best-effort "usage" for providers that don't have a CodexBar usage fetch
    strategy on this host. Derives a 5h window from Clawdbot transcripts and
    treats the spend within that window as % of a configured budget.
    """
    agents_dir = state_dir / "agents"
    if not agents_dir.is_dir():
        return None

    provider_value = str(provider or "").strip() or "unknown"
    window_minutes = int(window_minutes or 0)
    if window_minutes <= 0:
        window_minutes = 300

    budget = float(budget_usd or 0.0)
    if budget < 0 or budget != budget:
        budget = 0.0

    now_utc = datetime.now(timezone.utc)
    now_ts = now_utc.timestamp()
    window_s = float(window_minutes) * 60.0
    offset_s = float(int(reset_offset_minutes or 0) * 60)
    window_index = int((now_ts - offset_s) // window_s)
    window_start_s = window_index * window_s + offset_s
    window_end_s = window_start_s + window_s

    window_end_dt = datetime.fromtimestamp(window_end_s, tz=timezone.utc)
    resets_at = window_end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    totals: dict[str, float] = {
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    session_files = sorted(agents_dir.glob("*/sessions/*.jsonl"))
    for path in session_files:
        if path.name.endswith(".jsonl.lock"):
            continue
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < window_start_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue
                    if not isinstance(entry, dict):
                        continue
                    if entry.get("type") != "message":
                        continue
                    msg = entry.get("message")
                    if not isinstance(msg, dict):
                        continue
                    if msg.get("role") != "assistant":
                        continue

                    p = msg.get("provider") or entry.get("provider")
                    if str(p or "").strip().lower() != provider_value.lower():
                        continue

                    usage = msg.get("usage") or entry.get("usage")
                    if not isinstance(usage, dict):
                        continue

                    dt_utc = parse_iso_datetime(entry.get("timestamp")) or parse_iso_datetime(msg.get("timestamp"))
                    if dt_utc is None:
                        continue
                    ts = dt_utc.timestamp()
                    if ts < window_start_s or ts >= window_end_s:
                        continue

                    input_tokens = as_finite_number(usage.get("input"))
                    output_tokens = as_finite_number(usage.get("output"))
                    cache_read = as_finite_number(usage.get("cacheRead"))
                    cache_write = as_finite_number(usage.get("cacheWrite"))
                    total_tokens = as_finite_number(usage.get("totalTokens")) or as_finite_number(usage.get("total"))
                    if total_tokens is None:
                        total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                    cost_total = None
                    if provider_value.lower() == "minimax":
                        if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0
                        else:
                            cost_total = compute_minimax_cost_usd(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                cache_read_tokens=cache_read,
                                cache_write_tokens=cache_write,
                            )
                    else:
                        cost = usage.get("cost")
                        if isinstance(cost, dict):
                            cost_total = as_finite_number(cost.get("total"))
                        if cost_total is None:
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0

                    totals["totalTokens"] += total_tokens
                    totals["totalCost"] += cost_total
        except Exception as e:
            errors.append(
                {
                    "command": "clawdbot transcripts scan",
                    "provider": provider_value,
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Clawdbot sessions",
                }
            )

    used_percent = 0.0
    if budget > 0:
        used_percent = (totals["totalCost"] / budget) * 100.0

    login_method = f"Coding subscription (${budget:g})" if budget > 0 else "Coding subscription"

    return {
        "updatedAt": utc_now_iso(),
        "identity": {
            "loginMethod": login_method,
            "providerID": provider_value,
        },
        "primary": {
            "usedPercent": used_percent,
            "resetsAt": resets_at,
            "windowMinutes": window_minutes,
        },
        "secondary": None,
        "tertiary": None,
        "loginMethod": login_method,
        "window": {
            "budgetUSD": budget,
            "totalCostUSD": totals["totalCost"],
            "totalTokens": totals["totalTokens"],
            "missingCostEntries": totals["missingCostEntries"],
            "resetOffsetMinutes": int(reset_offset_minutes or 0),
        },
    }


def compute_direct_claude_minimax_usage(
    *,
    window_minutes: int,
    budget_usd: float,
    reset_offset_minutes: int,
    errors: list[dict[str, Any]],
    max_stderr_chars: int,
) -> dict[str, Any] | None:
    """
    Track MiniMax usage from direct Claude CLI sessions (not through Clawdbot).
    Scans ~/.claude/ directory for JSONL files with MiniMax model usage.
    """
    window_minutes = int(window_minutes or 0)
    if window_minutes <= 0:
        window_minutes = 300

    budget = float(budget_usd or 0.0)
    if budget < 0 or budget != budget:
        budget = 0.0

    now_utc = datetime.now(timezone.utc)
    now_ts = now_utc.timestamp()
    window_s = float(window_minutes) * 60.0
    offset_s = float(int(reset_offset_minutes or 0) * 60)
    window_index = int((now_ts - offset_s) // window_s)
    window_start_s = window_index * window_s + offset_s
    window_end_s = window_start_s + window_s

    window_end_dt = datetime.fromtimestamp(window_end_s, tz=timezone.utc)
    resets_at = window_end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    totals: dict[str, float] = {
        "totalTokens": 0.0,
        "totalCost": 0.0,
        "missingCostEntries": 0.0,
    }

    # Scan ~/.claude/ directory and any custom homeDirs used by Clawdbot
    claude_dirs = [Path.home() / ".claude"]

    # Add custom homeDir used by ClaudeMinimaxBot
    minimax_custom_home = Path("/home/web/.clawdbot-claude-bypass-home-minimax/.claude")
    if minimax_custom_home.is_dir():
        claude_dirs.append(minimax_custom_home)

    # Find all JSONL files in all claude directories
    jsonl_files = []
    for claude_dir in claude_dirs:
        if not claude_dir.is_dir():
            continue
        jsonl_files.extend(claude_dir.glob("*.jsonl"))
        jsonl_files.extend(claude_dir.glob("projects/*/*.jsonl"))

    if not jsonl_files:
        return None

    for path in sorted(jsonl_files):
        try:
            st = path.stat()
        except Exception:
            continue
        if st.st_mtime < window_start_s:
            continue

        try:
            with open(path, "r", encoding="utf-8", errors="replace") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        entry = json.loads(line)
                    except Exception:
                        continue

                    # Check for MiniMax model in different entry formats
                    model = None
                    usage = None
                    timestamp = None

                    # Format 1: Direct message entry
                    if entry.get("type") == "assistant" and isinstance(entry.get("message"), dict):
                        message = entry.get("message", {})
                        model = message.get("model")
                        usage = message.get("usage")
                        timestamp = entry.get("timestamp")
                    # Format 2: Project session entry
                    elif isinstance(entry.get("message"), dict):
                        message = entry.get("message", {})
                        model = message.get("model")
                        usage = message.get("usage")
                        timestamp = entry.get("timestamp")

                    # Check if this is MiniMax
                    if model and "minimax" in str(model).lower():
                        if not isinstance(usage, dict):
                            continue

                        dt_utc = parse_iso_datetime(timestamp)
                        if dt_utc is None:
                            continue
                        ts = dt_utc.timestamp()
                        if ts < window_start_s or ts >= window_end_s:
                            continue

                        # Extract tokens (different format from Clawdbot)
                        input_tokens = as_finite_number(usage.get("input_tokens"))
                        output_tokens = as_finite_number(usage.get("output_tokens"))
                        cache_read = as_finite_number(usage.get("cache_read_input_tokens"))
                        cache_write = as_finite_number(usage.get("cache_creation_input_tokens"))
                        total_tokens = as_finite_number(usage.get("total_tokens"))

                        if total_tokens is None:
                            total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                        # Calculate cost using MiniMax pricing
                        if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                            totals["missingCostEntries"] += 1.0
                            cost_total = 0.0
                        else:
                            cost_total = compute_minimax_cost_usd(
                                input_tokens=input_tokens,
                                output_tokens=output_tokens,
                                cache_read_tokens=cache_read,
                                cache_write_tokens=cache_write,
                            )

                        totals["totalTokens"] += total_tokens
                        totals["totalCost"] += cost_total

        except Exception as e:
            errors.append(
                {
                    "command": "direct claude cli scan",
                    "exitCode": 1,
                    "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Claude CLI logs",
                }
            )

    used_percent = 0.0
    if budget > 0:
        used_percent = (totals["totalCost"] / budget) * 100.0

    login_method = f"Coding subscription (${budget:g})" if budget > 0 else "Coding subscription"

    return {
        "updatedAt": utc_now_iso(),
        "identity": {
            "loginMethod": login_method,
            "providerID": "minimax",
        },
        "primary": {
            "usedPercent": used_percent,
            "resetsAt": resets_at,
            "windowMinutes": window_minutes,
        },
        "secondary": None,
        "tertiary": None,
        "loginMethod": login_method,
        "window": {
            "budgetUSD": budget,
            "totalCostUSD": totals["totalCost"],
            "totalTokens": totals["totalTokens"],
            "missingCostEntries": totals["missingCostEntries"],
            "resetOffsetMinutes": int(reset_offset_minutes or 0),
        },
    }


def merge_cost_items_by_provider(base: list[Any], delta: dict[str, Any], provider: str) -> None:
    if not delta:
        return

    target = None
    for item in base:
        if isinstance(item, dict) and item.get("provider") == provider:
            target = item
            break

    if target is None:
        base.append(delta)
        return

    # Merge totals + 30d totals
    target_totals = target.get("totals")
    if not isinstance(target_totals, dict):
        target_totals = {}
        target["totals"] = target_totals
    delta_totals = delta.get("totals")
    if isinstance(delta_totals, dict):
        for key in ["inputTokens", "outputTokens", "cacheReadTokens", "cacheCreationTokens", "totalTokens", "totalCost"]:
            a = as_finite_number(target_totals.get(key)) or 0.0
            b = as_finite_number(delta_totals.get(key)) or 0.0
            target_totals[key] = a + b

    for key in ["last30DaysTokens", "last30DaysCostUSD"]:
        a = as_finite_number(target.get(key)) or 0.0
        b = as_finite_number(delta.get(key)) or 0.0
        target[key] = a + b

    # Mark combined source to avoid confusion.
    base_source = str(target.get("source") or "").strip() or "local"
    delta_source = str(delta.get("source") or "").strip()
    if delta_source and delta_source not in base_source:
        target["source"] = f"{base_source}+{delta_source}"

    # Merge daily entries by date
    target_daily = target.get("daily")
    if not isinstance(target_daily, list):
        target_daily = []
        target["daily"] = target_daily
    by_date: dict[str, dict[str, Any]] = {}
    for d in target_daily:
        if isinstance(d, dict) and isinstance(d.get("date"), str):
            by_date[d["date"]] = d

    delta_daily = delta.get("daily")
    if isinstance(delta_daily, list):
        for d in delta_daily:
            if not isinstance(d, dict) or not isinstance(d.get("date"), str):
                continue
            date = d["date"]
            existing = by_date.get(date)
            if existing is None:
                target_daily.append(d)
                by_date[date] = d
                continue

            for key in ["inputTokens", "outputTokens", "cacheReadTokens", "cacheCreationTokens", "totalTokens", "totalCost"]:
                a = as_finite_number(existing.get(key)) or 0.0
                b = as_finite_number(d.get(key)) or 0.0
                existing[key] = a + b

            # Merge modelsUsed
            models_used = set(existing.get("modelsUsed") or []) if isinstance(existing.get("modelsUsed"), list) else set()
            models_used.update(d.get("modelsUsed") or [])
            existing["modelsUsed"] = sorted([m for m in models_used if isinstance(m, str) and m])

            # Merge modelBreakdowns
            def to_cost_map(items: Any) -> dict[str, float]:
                out: dict[str, float] = {}
                if not isinstance(items, list):
                    return out
                for mb in items:
                    if not isinstance(mb, dict):
                        continue
                    name = mb.get("modelName")
                    cost = as_finite_number(mb.get("cost")) or 0.0
                    if isinstance(name, str) and name:
                        out[name] = out.get(name, 0.0) + cost
                return out

            merged = to_cost_map(existing.get("modelBreakdowns"))
            for name, cost in to_cost_map(d.get("modelBreakdowns")).items():
                merged[name] = merged.get(name, 0.0) + cost
            existing["modelBreakdowns"] = [
                {"modelName": name, "cost": cost} for name, cost in sorted(merged.items(), key=lambda kv: kv[1], reverse=True)
            ]

    target_daily.sort(key=lambda d: d.get("date") or "")


def update_hourly_history(data_dir: Path, usage_items: list[dict[str, Any]], generated_at: str, current_codex_account: str | None = None) -> None:
    """
    Track hourly usage history for heatmap visualization.
    Calculates delta (activity) by comparing current values to previous run.
    Each entry: {ts, hour, day, provider, account, activity}
    Keep last 30 days of data.
    """
    history_path = data_dir / "history.json"
    state_path = data_dir / ".history_state.json"

    history = load_history(history_path)
    prev_state = load_history(state_path) if state_path.is_file() else {}
    if isinstance(prev_state, list):
        prev_state = {}

    # Use local time for hour/day display
    now_local = datetime.now()
    now = datetime.now(timezone.utc)
    hour = now_local.hour
    day = now_local.strftime("%Y-%m-%d")

    current_state: dict[str, Any] = {}

    # Extract usage percentages and calculate deltas
    for item in usage_items:
        if not isinstance(item, dict):
            continue

        provider = item.get("provider", "unknown")
        account = item.get("codexAuthAccount", "")

        usage = item.get("usage", {})
        if not isinstance(usage, dict):
            continue

        primary = usage.get("primary", {})
        session_pct = primary.get("usedPercent") if isinstance(primary, dict) else None

        if session_pct is None:
            continue

        key = f"{provider}|{account}"
        # Always update state for all accounts (needed for accurate delta calculation on switch)
        current_state[key] = {"sessionPct": session_pct, "ts": generated_at}

        # Calculate activity delta
        prev = prev_state.get(key, {})
        prev_pct = prev.get("sessionPct")

        activity = 0.0
        if prev_pct is not None and session_pct is not None:
            # If session reset (current < previous), count current as new activity
            if session_pct < prev_pct - 0.5:  # Allow small tolerance for reset detection
                activity = session_pct
            else:
                activity = session_pct - prev_pct
        elif session_pct is not None and session_pct > 0:
            # First run or no previous data - if there's usage, record it
            activity = session_pct

        # For codex provider, only record history for the current active account
        if provider == "codex" and current_codex_account and account != current_codex_account:
            continue

        # Only record if there was meaningful activity (> 0.1% to avoid noise)
        if activity > 0.1:
            entry = {
                "ts": generated_at,
                "hour": hour,
                "day": day,
                "provider": provider,
                "account": account,
                "activity": round(activity, 1),
            }
            history.append(entry)

    # Save current state for next comparison
    atomic_write_json(state_path, current_state)

    # Prune entries older than 30 days
    cutoff = (now - timedelta(days=30)).strftime("%Y-%m-%d")
    history = [e for e in history if e.get("day", "") >= cutoff]

    atomic_write_json(history_path, history)


def parse_accounts_env(value: str) -> list[str]:
    value = value.strip()
    if not value:
        return []
    return [part for part in re.split(r"[,\s]+", value) if part]


def remove_key_recursive(obj: Any, key: str) -> None:
    if isinstance(obj, dict):
        if key in obj:
            obj.pop(key, None)
        for v in list(obj.values()):
            remove_key_recursive(v, key)
    elif isinstance(obj, list):
        for v in obj:
            remove_key_recursive(v, key)


def main() -> int:
    os.umask(0o022)

    base_dir = Path(__file__).resolve().parent.parent
    data_dir = base_dir / "public" / "data"

    base_env = ensure_path_has_dir(dict(os.environ), str(Path.home() / ".local" / "bin"))

    codexbar_bin = os.environ.get("CODEXBAR_BIN", "/usr/local/bin/codexbar")
    usage_timeout_s = int(os.environ.get("USAGE_TIMEOUT_S", "60"))
    cost_timeout_s = int(os.environ.get("COST_TIMEOUT_S", "180"))
    max_stderr_chars = int(os.environ.get("MAX_STDERR_CHARS", "4000"))

    codex_accounts_dir = Path(os.environ.get("CODEX_ACCOUNTS_DIR", str(Path.home() / ".codex" / "accounts")))
    codex_config_toml = Path(os.environ.get("CODEX_CONFIG_TOML", str(Path.home() / ".codex" / "config.toml")))
    selected_accounts = parse_accounts_env(os.environ.get("CODEX_AUTH_ACCOUNTS", ""))

    generated_at = utc_now_iso()
    hostname = socket.gethostname()

    # Get current active codex account
    current_codex_account = None
    current_file = Path.home() / ".codex" / "current"
    if current_file.is_file():
        try:
            current_codex_account = current_file.read_text().strip()
        except Exception:
            pass

    errors: list[dict[str, Any]] = []
    usage_items: list[dict[str, Any]] = []
    cost_items: list[Any] = []

    exit_codes: dict[str, Any] = {"usage": {"codex": {}, "claude": None, "gemini": None, "minimax": None}, "cost": None}

    minimax_state_dir = Path(os.environ.get("CLAWDBOT_MINIMAX_STATE_DIR", str(Path.home() / ".clawdbot-minimax")))
    try:
        minimax_session_budget_usd = float(os.environ.get("MINIMAX_SESSION_BUDGET_USD", "50"))
    except Exception:
        minimax_session_budget_usd = 50.0
    try:
        minimax_session_window_minutes = int(os.environ.get("MINIMAX_SESSION_WINDOW_MINUTES", "300"))
    except Exception:
        minimax_session_window_minutes = 300
    try:
        minimax_session_reset_offset_minutes = int(os.environ.get("MINIMAX_SESSION_RESET_OFFSET_MINUTES", "0"))
    except Exception:
        minimax_session_reset_offset_minutes = 0

    # --- Codex: multi-account via codex-auth profiles (isolated HOME per profile; no global switching).
    all_account_files = sorted(codex_accounts_dir.glob("*.json")) if codex_accounts_dir.is_dir() else []
    if selected_accounts:
        by_name = {p.stem: p for p in all_account_files}
        account_files: list[Path] = []
        for name in selected_accounts:
            p = by_name.get(name)
            if p is None:
                errors.append(
                    {
                        "command": "codexbar usage --provider codex",
                        "profile": name,
                        "exitCode": 1,
                        "message": f"codex-auth profile not found: {codex_accounts_dir}/{name}.json",
                    }
                )
                continue
            account_files.append(p)
    else:
        account_files = all_account_files

    if not account_files:
        errors.append(
            {
                "command": "codexbar usage --provider codex",
                "exitCode": 1,
                "message": f"No codex-auth accounts found in {codex_accounts_dir}",
            }
        )

    for account_file in account_files:
        profile = account_file.stem
        with tempfile.TemporaryDirectory(prefix="codex-home-") as tmp_home:
            tmp_home_path = Path(tmp_home)
            tmp_codex_dir = tmp_home_path / ".codex"
            tmp_codex_dir.mkdir(parents=True, exist_ok=True)
            (tmp_codex_dir / "auth.json").symlink_to(account_file)
            if codex_config_toml.is_file():
                shutil.copy2(codex_config_toml, tmp_codex_dir / "config.toml")

            env = dict(base_env)
            env["HOME"] = tmp_home

            res = run_cmd(
                [codexbar_bin, "usage", "--provider", "codex", "--format", "json", "--json-only"],
                timeout_s=usage_timeout_s,
                env=env,
            )

        exit_codes["usage"]["codex"][profile] = res.exit_code

        if res.stderr.strip() or res.exit_code != 0:
            errors.append(
                {
                    "command": "codexbar usage --provider codex",
                    "profile": profile,
                    "exitCode": res.exit_code,
                    "message": safe_tail(res.stderr, max_stderr_chars) or f"Exit {res.exit_code}",
                }
            )

        parsed = parse_json_arrays(res.stdout, errors=errors, context=f"usage.codex.{profile}")
        for item in parsed:
            if isinstance(item, dict) and item.get("provider") != "cli":
                item["codexAuthAccount"] = profile
                usage_items.append(item)

    # --- Claude usage (OAuth, configured in ~/.codexbar/config.json)
    claude_res = run_cmd(
        [codexbar_bin, "usage", "--provider", "claude", "--format", "json", "--json-only"],
        timeout_s=usage_timeout_s,
        env=base_env,
    )
    if is_claude_oauth_token_expired(claude_res):
        refresh_claude_oauth_token(errors=errors, max_stderr_chars=max_stderr_chars)
        claude_res = run_cmd(
            [codexbar_bin, "usage", "--provider", "claude", "--format", "json", "--json-only"],
            timeout_s=usage_timeout_s,
            env=base_env,
        )
    exit_codes["usage"]["claude"] = claude_res.exit_code
    claude_parsed = parse_json_arrays(claude_res.stdout, errors=errors, context="usage.claude")
    claude_has_provider_error = any(isinstance(item, dict) and isinstance(item.get("error"), dict) for item in claude_parsed)
    if claude_res.stderr.strip() or (claude_res.exit_code != 0 and not claude_has_provider_error):
        errors.append(
            {
                "command": "codexbar usage --provider claude",
                "exitCode": claude_res.exit_code,
                "message": safe_tail(claude_res.stderr, max_stderr_chars) or f"Exit {claude_res.exit_code}",
            }
        )
    for item in claude_parsed:
        if isinstance(item, dict) and item.get("provider") != "cli":
            usage_items.append(item)

    # --- Gemini usage (API via Gemini CLI credentials)
    gemini_res = run_cmd(
        [codexbar_bin, "usage", "--provider", "gemini", "--source", "api", "--format", "json", "--json-only"],
        timeout_s=usage_timeout_s,
        env=base_env,
    )
    exit_codes["usage"]["gemini"] = gemini_res.exit_code
    gemini_parsed = parse_json_arrays(gemini_res.stdout, errors=errors, context="usage.gemini")
    gemini_has_provider_error = any(isinstance(item, dict) and isinstance(item.get("error"), dict) for item in gemini_parsed)
    if gemini_res.stderr.strip() or (gemini_res.exit_code != 0 and not gemini_has_provider_error):
        errors.append(
            {
                "command": "codexbar usage --provider gemini",
                "exitCode": gemini_res.exit_code,
                "message": safe_tail(gemini_res.stderr, max_stderr_chars) or f"Exit {gemini_res.exit_code}",
            }
        )
    for item in gemini_parsed:
        if isinstance(item, dict) and item.get("provider") != "cli":
            usage_items.append(item)

    # --- MiniMax usage from Clawdbot profile: `clawdbot --profile minimax`
    minimax_usage = compute_clawdbot_window_usage(
        provider="minimax",
        state_dir=minimax_state_dir,
        window_minutes=minimax_session_window_minutes,
        budget_usd=minimax_session_budget_usd,
        reset_offset_minutes=minimax_session_reset_offset_minutes,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )

    # --- MiniMax usage from direct Claude CLI: `claude --model minimax`
    direct_minimax_usage = compute_direct_claude_minimax_usage(
        window_minutes=minimax_session_window_minutes,
        budget_usd=minimax_session_budget_usd,
        reset_offset_minutes=minimax_session_reset_offset_minutes,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )

    # Merge both Clawdbot and direct CLI usage
    if minimax_usage or direct_minimax_usage:
        combined_usage = {
            "provider": "minimax",
            "source": "claude-cli+clawdbot",
            "version": "both",
            "usage": {}
        }

        # Combine usage data
        if minimax_usage and direct_minimax_usage:
            # Merge percentages and costs
            claw_pct = minimax_usage.get("primary", {}).get("usedPercent", 0)
            direct_pct = direct_minimax_usage.get("primary", {}).get("usedPercent", 0)
            combined_pct = claw_pct + direct_pct

            combined_usage["usage"] = {
                "primary": {
                    "usedPercent": combined_pct,
                    "resetsAt": minimax_usage.get("primary", {}).get("resetsAt"),
                    "windowMinutes": minimax_session_window_minutes,
                },
                "secondary": None,
                "tertiary": None,
                "loginMethod": "Coding subscription (${budget:g})",
                "identity": {
                    "loginMethod": "Coding subscription (${budget:g})",
                    "providerID": "minimax",
                }
            }
        elif minimax_usage:
            combined_usage["usage"] = minimax_usage
        else:
            combined_usage["usage"] = direct_minimax_usage

        usage_items.append(combined_usage)
        exit_codes["usage"]["minimax"] = 0
    elif minimax_usage:
        usage_items.append(
            {
                "provider": "minimax",
                "source": "clawdbot-minimax",
                "version": "clawdbot",
                "usage": minimax_usage,
            }
        )
        exit_codes["usage"]["minimax"] = 0
    elif direct_minimax_usage:
        usage_items.append(
            {
                "provider": "minimax",
                "source": "direct-cli",
                "version": "direct",
                "usage": direct_minimax_usage,
            }
        )
        exit_codes["usage"]["minimax"] = 0

    # Record provider-level JSON errors even if the command exited 0.
    for item in usage_items:
        if not isinstance(item, dict):
            continue
        err = item.get("error")
        if not isinstance(err, dict):
            continue
        errors.append(
            {
                "command": "codexbar usage",
                "provider": item.get("provider"),
                "profile": item.get("codexAuthAccount"),
                "exitCode": err.get("code", 0),
                "message": err.get("message", "Provider error"),
            }
        )

    # Remove any emails before publishing (public endpoint).
    remove_key_recursive(usage_items, "accountEmail")

    # --- Cost (local logs; Codex + Claude only)
    #
    # CodexBar groups daily totals by the process timezone. The dashboards need:
    # - EN: New York day boundaries
    # - RU: Minsk day boundaries
    #
    # We compute both and publish them so the frontend can switch based on language.
    def run_cost_for_lang(lang: str, tz_name: str, *, refresh: bool) -> CmdResult:
        env = dict(base_env)
        env["TZ"] = tz_name
        args = [codexbar_bin, "cost", "--provider", "both", "--format", "json", "--json-only"]
        if refresh:
            args.append("--refresh")
        return run_cmd(args, timeout_s=cost_timeout_s, env=env)

    # Refresh scan results on the first run to avoid stale "Today" totals.
    cost_res_en = run_cost_for_lang("en", "America/New_York", refresh=True)
    cost_res_ru = run_cost_for_lang("ru", "Europe/Minsk", refresh=False)

    exit_codes["cost"] = {"en": cost_res_en.exit_code, "ru": cost_res_ru.exit_code}

    if cost_res_en.stderr.strip() or cost_res_en.exit_code != 0:
        errors.append(
            {
                "command": "codexbar cost --provider both",
                "lang": "en",
                "tz": "America/New_York",
                "exitCode": cost_res_en.exit_code,
                "message": safe_tail(cost_res_en.stderr, max_stderr_chars) or f"Exit {cost_res_en.exit_code}",
            }
        )
    if cost_res_ru.stderr.strip() or cost_res_ru.exit_code != 0:
        errors.append(
            {
                "command": "codexbar cost --provider both",
                "lang": "ru",
                "tz": "Europe/Minsk",
                "exitCode": cost_res_ru.exit_code,
                "message": safe_tail(cost_res_ru.stderr, max_stderr_chars) or f"Exit {cost_res_ru.exit_code}",
            }
        )

    cost_items_en = parse_json_arrays(cost_res_en.stdout, errors=errors, context="cost.en")
    cost_items_ru = parse_json_arrays(cost_res_ru.stdout, errors=errors, context="cost.ru")
    cost_items_en = [c for c in cost_items_en if isinstance(c, dict) and c.get("provider") != "cli"]
    cost_items_ru = [c for c in cost_items_ru if isinstance(c, dict) and c.get("provider") != "cli"]

    # --- Clawdbot Claude cost (local session transcripts)
    #
    # Clawdbot uses `claude -p --output-format json ...` and records per-message
    # usage/cost into ~/.clawdbot session transcripts. CodexBar doesn't scan
    # those files, so we merge them into the Claude cost provider.
    claw_cost_en = compute_clawdbot_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="claude",
        source="clawdbot",
        match_providers=("anthropic", "claude"),
        match_model_prefixes=("claude-",),
    )
    claw_cost_ru = compute_clawdbot_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="claude",
        source="clawdbot",
        match_providers=("anthropic", "claude"),
        match_model_prefixes=("claude-",),
    )
    if claw_cost_en:
        merge_cost_items_by_provider(cost_items_en, claw_cost_en, "claude")
    if claw_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, claw_cost_ru, "claude")

    # --- Clawdbot MiniMax cost (Clawdbot profile: `clawdbot --profile minimax`)
    minimax_cost_en = compute_clawdbot_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="minimax",
        source="clawdbot-minimax",
        state_dir=minimax_state_dir,
        match_providers=("minimax",),
    )
    minimax_cost_ru = compute_clawdbot_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
        provider="minimax",
        source="clawdbot-minimax",
        state_dir=minimax_state_dir,
        match_providers=("minimax",),
    )
    if minimax_cost_en:
        merge_cost_items_by_provider(cost_items_en, minimax_cost_en, "minimax")
    if minimax_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, minimax_cost_ru, "minimax")

    # --- Direct CLI MiniMax cost (scans ~/.claude/ for direct MiniMax usage)
    #
    # Direct Claude CLI with MiniMax model stores usage in ~/.claude/ JSONL files.
    # We scan these to get cost data for the direct CLI usage.
    def compute_direct_cli_minimax_cost_delta(
        *,
        tz_name: str,
        days: int,
        errors: list[dict[str, Any]],
        max_stderr_chars: int,
    ) -> dict[str, Any] | None:
        """
        Track MiniMax cost from direct Claude CLI sessions.
        Scans ~/.claude/ directory for JSONL files with MiniMax model usage.
        """
        now_utc = datetime.now(timezone.utc)
        since_utc = now_utc - timedelta(days=max(1, days) - 1)
        since_ts_s = since_utc.timestamp()

        daily: dict[str, dict[str, Any]] = {}
        totals: dict[str, float] = {
            "inputTokens": 0.0,
            "outputTokens": 0.0,
            "cacheReadTokens": 0.0,
            "cacheCreationTokens": 0.0,
            "totalTokens": 0.0,
            "totalCost": 0.0,
            "missingCostEntries": 0.0,
        }

        # Scan ~/.claude/ directory and any custom homeDirs used by Clawdbot
        claude_dirs = [Path.home() / ".claude"]

        # Add custom homeDir used by ClaudeMinimaxBot
        minimax_custom_home = Path("/home/web/.clawdbot-claude-bypass-home-minimax/.claude")
        if minimax_custom_home.is_dir():
            claude_dirs.append(minimax_custom_home)

        # Find all JSONL files in all claude directories
        jsonl_files = []
        for claude_dir in claude_dirs:
            if not claude_dir.is_dir():
                continue
            jsonl_files.extend(claude_dir.glob("*.jsonl"))
            jsonl_files.extend(claude_dir.glob("projects/*/*.jsonl"))

        if not jsonl_files:
            return None

        for path in sorted(jsonl_files):
            try:
                st = path.stat()
            except Exception:
                continue
            if st.st_mtime < since_ts_s:
                continue

            try:
                with open(path, "r", encoding="utf-8", errors="replace") as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            entry = json.loads(line)
                        except Exception:
                            continue

                        # Check for MiniMax model in different entry formats
                        model = None
                        usage = None
                        timestamp = None

                        # Format 1: Direct message entry
                        if entry.get("type") == "assistant" and isinstance(entry.get("message"), dict):
                            message = entry.get("message", {})
                            model = message.get("model")
                            usage = message.get("usage")
                            timestamp = entry.get("timestamp")
                        # Format 2: Project session entry
                        elif isinstance(entry.get("message"), dict):
                            message = entry.get("message", {})
                            model = message.get("model")
                            usage = message.get("usage")
                            timestamp = entry.get("timestamp")

                        # Check if this is MiniMax
                        if model and "minimax" in str(model).lower():
                            if not isinstance(usage, dict):
                                continue

                            dt_utc = parse_iso_datetime(timestamp)
                            if dt_utc is None or dt_utc.timestamp() < since_ts_s:
                                continue
                            day = to_ymd_in_tz(dt_utc, tz_name)
                            if not day:
                                continue

                            bucket = daily.get(day)
                            if bucket is None:
                                bucket = {
                                    "date": day,
                                    "inputTokens": 0.0,
                                    "outputTokens": 0.0,
                                    "cacheReadTokens": 0.0,
                                    "cacheCreationTokens": 0.0,
                                    "totalTokens": 0.0,
                                    "totalCost": 0.0,
                                    "missingCostEntries": 0.0,
                                    "modelsUsed": set(),
                                    "modelCosts": {},
                                }
                                daily[day] = bucket

                            # Extract tokens (different format from Clawdbot)
                            input_tokens = as_finite_number(usage.get("input_tokens"))
                            output_tokens = as_finite_number(usage.get("output_tokens"))
                            cache_read = as_finite_number(usage.get("cache_read_input_tokens"))
                            cache_write = as_finite_number(usage.get("cache_creation_input_tokens"))
                            total_tokens = as_finite_number(usage.get("total_tokens"))

                            if total_tokens is None:
                                total_tokens = (input_tokens or 0.0) + (output_tokens or 0.0) + (cache_read or 0.0) + (cache_write or 0.0)

                            # Calculate cost
                            if input_tokens is None and output_tokens is None and cache_read is None and cache_write is None:
                                bucket["missingCostEntries"] += 1.0
                                totals["missingCostEntries"] += 1.0
                                cost_total = 0.0
                            else:
                                cost_total = compute_minimax_cost_usd(
                                    input_tokens=input_tokens,
                                    output_tokens=output_tokens,
                                    cache_read_tokens=cache_read,
                                    cache_write_tokens=cache_write,
                                )

                            bucket["inputTokens"] += input_tokens or 0.0
                            bucket["outputTokens"] += output_tokens or 0.0
                            bucket["cacheReadTokens"] += cache_read or 0.0
                            bucket["cacheCreationTokens"] += cache_write or 0.0
                            bucket["totalTokens"] += total_tokens
                            bucket["totalCost"] += cost_total

                            totals["inputTokens"] += input_tokens or 0.0
                            totals["outputTokens"] += output_tokens or 0.0
                            totals["cacheReadTokens"] += cache_read or 0.0
                            totals["cacheCreationTokens"] += cache_write or 0.0
                            totals["totalTokens"] += total_tokens
                            totals["totalCost"] += cost_total

                            if isinstance(model, str) and model:
                                bucket["modelsUsed"].add(model)
                                model_costs = bucket["modelCosts"]
                                prev_cost = model_costs.get(model, 0.0)
                                model_costs[model] = prev_cost + cost_total

            except Exception as e:
                errors.append(
                    {
                        "command": "direct claude cli cost scan",
                        "exitCode": 1,
                        "message": safe_tail(str(e), max_stderr_chars) or "Failed to scan Direct CLI logs",
                    }
                )

        if not daily:
            return None

        daily_items: list[dict[str, Any]] = []
        for day in sorted(daily.keys()):
            b = daily[day]
            model_breakdowns = [
                {"modelName": name, "cost": cost} for name, cost in sorted(b["modelCosts"].items(), key=lambda kv: kv[1], reverse=True)
            ]
            models_used = sorted(b["modelsUsed"])
            daily_items.append(
                {
                    "date": b["date"],
                    "inputTokens": b["inputTokens"],
                    "outputTokens": b["outputTokens"],
                    "cacheReadTokens": b["cacheReadTokens"],
                    "cacheCreationTokens": b["cacheCreationTokens"],
                    "totalTokens": b["totalTokens"],
                    "totalCost": b["totalCost"],
                    "modelBreakdowns": model_breakdowns,
                    "modelsUsed": models_used,
                }
                )

        return {
            "updatedAt": utc_now_iso(),
            "provider": "minimax",
            "source": "direct-cli",
            "totals": {
                "inputTokens": totals["inputTokens"],
                "outputTokens": totals["outputTokens"],
                "cacheReadTokens": totals["cacheReadTokens"],
                "cacheCreationTokens": totals["cacheCreationTokens"],
                "totalTokens": totals["totalTokens"],
                "totalCost": totals["totalCost"],
            },
            "daily": daily_items,
            "last30DaysTokens": totals["totalTokens"],
            "last30DaysCostUSD": totals["totalCost"],
        }

    direct_minimax_cost_en = compute_direct_cli_minimax_cost_delta(
        tz_name="America/New_York",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )
    direct_minimax_cost_ru = compute_direct_cli_minimax_cost_delta(
        tz_name="Europe/Minsk",
        days=30,
        errors=errors,
        max_stderr_chars=max_stderr_chars,
    )
    if direct_minimax_cost_en:
        merge_cost_items_by_provider(cost_items_en, direct_minimax_cost_en, "minimax")
    if direct_minimax_cost_ru:
        merge_cost_items_by_provider(cost_items_ru, direct_minimax_cost_ru, "minimax")

    # Back-compat: keep the original `cost` field in EN (New York).
    cost_items = cost_items_en

    # Best-effort cleanup for rare cases where CodexBar launches a Claude CLI session
    # that detaches and keeps running.
    run_codexbar_claude_guard()

    latest = {
        "generatedAt": generated_at,
        "hostname": hostname,
        "currentCodexAccount": current_codex_account,
        "usage": usage_items,
        "cost": cost_items,
        "costByLang": {
            "en": cost_items_en,
            "ru": cost_items_ru,
        },
        "errors": errors,
    }

    last_run = {
        "generatedAt": generated_at,
        "ok": len(errors) == 0,
        "exitCodes": exit_codes,
        "errors": errors,
    }

    atomic_write_json(data_dir / "latest.json", latest)
    atomic_write_json(data_dir / "last-run.json", last_run)

    # Update hourly history for heatmap (only current codex account)
    update_hourly_history(data_dir, usage_items, generated_at, current_codex_account)

    return 0 if last_run["ok"] else 0


if __name__ == "__main__":
    raise SystemExit(main())
